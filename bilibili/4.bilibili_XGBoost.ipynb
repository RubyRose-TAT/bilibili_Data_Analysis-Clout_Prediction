{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bb496b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from xgboost import plot_importance,plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use({'figure.figsize':(15,10)})\n",
    "\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f949ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bilibili_rank100_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2f3f3",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d681de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#按标题去重\n",
    "df = df.drop_duplicates(subset=['title'],keep='first',inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3526775c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>tag</th>\n",
       "      <th>partition</th>\n",
       "      <th>funs</th>\n",
       "      <th>like</th>\n",
       "      <th>coins</th>\n",
       "      <th>collect</th>\n",
       "      <th>share</th>\n",
       "      <th>danmu</th>\n",
       "      <th>reply</th>\n",
       "      <th>time</th>\n",
       "      <th>like_rate</th>\n",
       "      <th>views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>星有野</td>\n",
       "      <td>【我怎么才能让你相信国创动画】</td>\n",
       "      <td>NaN</td>\n",
       "      <td>guochuang</td>\n",
       "      <td>2061928</td>\n",
       "      <td>253632</td>\n",
       "      <td>112476</td>\n",
       "      <td>42338</td>\n",
       "      <td>8076</td>\n",
       "      <td>6495</td>\n",
       "      <td>5382</td>\n",
       "      <td>109.300000</td>\n",
       "      <td>0.105747</td>\n",
       "      <td>2398481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5503</th>\n",
       "      <td>是阿胜呀-</td>\n",
       "      <td>熊出没四个穿帮镜头</td>\n",
       "      <td>NaN</td>\n",
       "      <td>guochuang</td>\n",
       "      <td>325</td>\n",
       "      <td>994</td>\n",
       "      <td>71</td>\n",
       "      <td>629</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>161.476873</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5504</th>\n",
       "      <td>次元小主</td>\n",
       "      <td>圆满了！时隔16年，官方终于出又大电影了！</td>\n",
       "      <td>NaN</td>\n",
       "      <td>guochuang</td>\n",
       "      <td>107330</td>\n",
       "      <td>19904</td>\n",
       "      <td>3049</td>\n",
       "      <td>13581</td>\n",
       "      <td>122</td>\n",
       "      <td>172</td>\n",
       "      <td>181</td>\n",
       "      <td>166.302921</td>\n",
       "      <td>0.012275</td>\n",
       "      <td>1621472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5507</th>\n",
       "      <td>-_艾达王_-</td>\n",
       "      <td>《尸兄59》白小飞蜕变为白龙守护者！尸王了结心路历程！人类集结所有战力和尸王决战！！最终大战...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>guochuang</td>\n",
       "      <td>74638</td>\n",
       "      <td>11862</td>\n",
       "      <td>9127</td>\n",
       "      <td>2028</td>\n",
       "      <td>67</td>\n",
       "      <td>404</td>\n",
       "      <td>466</td>\n",
       "      <td>1.323056</td>\n",
       "      <td>0.095506</td>\n",
       "      <td>124202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5518</th>\n",
       "      <td>龙珠哥哥呀</td>\n",
       "      <td>智者的对决，使用脑发达药水后，智商提升100倍，舌战相当激烈！</td>\n",
       "      <td>NaN</td>\n",
       "      <td>guochuang</td>\n",
       "      <td>9541</td>\n",
       "      <td>14408</td>\n",
       "      <td>31</td>\n",
       "      <td>675</td>\n",
       "      <td>8</td>\n",
       "      <td>91</td>\n",
       "      <td>160</td>\n",
       "      <td>5.175903</td>\n",
       "      <td>0.063780</td>\n",
       "      <td>225902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7253</th>\n",
       "      <td>热点库</td>\n",
       "      <td>11岁男孩掏出攒的钱，妈妈瞬间惊呆，网友：成年人都没这么多</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rookie</td>\n",
       "      <td>1035</td>\n",
       "      <td>32260</td>\n",
       "      <td>37</td>\n",
       "      <td>1386</td>\n",
       "      <td>145</td>\n",
       "      <td>96</td>\n",
       "      <td>1318</td>\n",
       "      <td>5.711146</td>\n",
       "      <td>0.045203</td>\n",
       "      <td>713670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7269</th>\n",
       "      <td>生活纪实录</td>\n",
       "      <td>女子为证清白主动要求做DNA，报告出来后，自己都不知谁是孩子爹</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rookie</td>\n",
       "      <td>1242</td>\n",
       "      <td>14821</td>\n",
       "      <td>722</td>\n",
       "      <td>2385</td>\n",
       "      <td>7891</td>\n",
       "      <td>2832</td>\n",
       "      <td>2998</td>\n",
       "      <td>5.794375</td>\n",
       "      <td>0.017965</td>\n",
       "      <td>825002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>大富翁酒吧老板-阿杰</td>\n",
       "      <td>我是不是最惨的酒吧老板。我在杭州滨江花了300w开了这么一家酒吧</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rookie</td>\n",
       "      <td>3716</td>\n",
       "      <td>37489</td>\n",
       "      <td>504</td>\n",
       "      <td>3536</td>\n",
       "      <td>1831</td>\n",
       "      <td>828</td>\n",
       "      <td>2566</td>\n",
       "      <td>4.975764</td>\n",
       "      <td>0.034894</td>\n",
       "      <td>1074373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7291</th>\n",
       "      <td>bili_64364371448</td>\n",
       "      <td>江西某211某食堂木桶饭现状</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rookie</td>\n",
       "      <td>26</td>\n",
       "      <td>7859</td>\n",
       "      <td>46</td>\n",
       "      <td>530</td>\n",
       "      <td>1027</td>\n",
       "      <td>130</td>\n",
       "      <td>1086</td>\n",
       "      <td>4.596424</td>\n",
       "      <td>0.018683</td>\n",
       "      <td>420649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7300</th>\n",
       "      <td>科学融媒</td>\n",
       "      <td>女生撕开雪糕一看，直呼实物和图片符合，网友：良心商家</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rookie</td>\n",
       "      <td>61</td>\n",
       "      <td>3859</td>\n",
       "      <td>16</td>\n",
       "      <td>224</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>194</td>\n",
       "      <td>3.646644</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>328656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>113 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                author                                              title  \\\n",
       "158                星有野                                    【我怎么才能让你相信国创动画】   \n",
       "5503             是阿胜呀-                                          熊出没四个穿帮镜头   \n",
       "5504              次元小主                              圆满了！时隔16年，官方终于出又大电影了！   \n",
       "5507           -_艾达王_-  《尸兄59》白小飞蜕变为白龙守护者！尸王了结心路历程！人类集结所有战力和尸王决战！！最终大战...   \n",
       "5518             龙珠哥哥呀                    智者的对决，使用脑发达药水后，智商提升100倍，舌战相当激烈！   \n",
       "...                ...                                                ...   \n",
       "7253               热点库                      11岁男孩掏出攒的钱，妈妈瞬间惊呆，网友：成年人都没这么多   \n",
       "7269             生活纪实录                    女子为证清白主动要求做DNA，报告出来后，自己都不知谁是孩子爹   \n",
       "7270        大富翁酒吧老板-阿杰                   我是不是最惨的酒吧老板。我在杭州滨江花了300w开了这么一家酒吧   \n",
       "7291  bili_64364371448                                     江西某211某食堂木桶饭现状   \n",
       "7300              科学融媒                         女生撕开雪糕一看，直呼实物和图片符合，网友：良心商家   \n",
       "\n",
       "      tag  partition     funs    like   coins  collect  share  danmu  reply  \\\n",
       "158   NaN  guochuang  2061928  253632  112476    42338   8076   6495   5382   \n",
       "5503  NaN  guochuang      325     994      71      629      9     10     10   \n",
       "5504  NaN  guochuang   107330   19904    3049    13581    122    172    181   \n",
       "5507  NaN  guochuang    74638   11862    9127     2028     67    404    466   \n",
       "5518  NaN  guochuang     9541   14408      31      675      8     91    160   \n",
       "...   ...        ...      ...     ...     ...      ...    ...    ...    ...   \n",
       "7253  NaN     rookie     1035   32260      37     1386    145     96   1318   \n",
       "7269  NaN     rookie     1242   14821     722     2385   7891   2832   2998   \n",
       "7270  NaN     rookie     3716   37489     504     3536   1831    828   2566   \n",
       "7291  NaN     rookie       26    7859      46      530   1027    130   1086   \n",
       "7300  NaN     rookie       61    3859      16      224     26      6    194   \n",
       "\n",
       "            time  like_rate    views  \n",
       "158   109.300000   0.105747  2398481  \n",
       "5503  161.476873   0.001262   787500  \n",
       "5504  166.302921   0.012275  1621472  \n",
       "5507    1.323056   0.095506   124202  \n",
       "5518    5.175903   0.063780   225902  \n",
       "...          ...        ...      ...  \n",
       "7253    5.711146   0.045203   713670  \n",
       "7269    5.794375   0.017965   825002  \n",
       "7270    4.975764   0.034894  1074373  \n",
       "7291    4.596424   0.018683   420649  \n",
       "7300    3.646644   0.011742   328656  \n",
       "\n",
       "[113 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#缺失值处理\n",
    "df = df.drop(df[df['time']>1000].index)\n",
    "df[df.isnull().values==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13859d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#重置索引\n",
    "df=df.reset_index(drop=True, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae547009",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#分区按序号编码\n",
    "def LabelEncoding(df):\n",
    "    x, dfc = 'partition', df\n",
    "    key = dfc[x].unique()  # 将唯一值作为关键字\n",
    "    value = [i for i in range(len(key))]  # 键值\n",
    "    Dict = dict(zip(key, value))  # 字典，即键值对\n",
    "    for i in range(len(key)):\n",
    "        for j in range(dfc.shape[0]):\n",
    "            if key[i] == dfc[x][j]:\n",
    "                dfc[x][j] = Dict[key[i]]\n",
    "    dfc[x] = dfc[x].astype(np.int64)\n",
    "    return dfc\n",
    "\n",
    "df = LabelEncoding(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bdbaa63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5905, 14)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311f1e3",
   "metadata": {},
   "source": [
    "## 划分训练集测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "762ce0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"author\",\"title\",\"tag\"],axis = 1)\n",
    "X = df.drop([\"views\"],axis = 1)\n",
    "y = df[\"views\"]\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e57781c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4428, 10), (1477, 10))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa91c56",
   "metadata": {},
   "source": [
    "## 调整XGBoost的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f618696c",
   "metadata": {},
   "source": [
    "首先，调整 `max_depth` 和 `min_child_weight` 参数。稍后，我们将用一个较小的网格来完善这两个选择。我们将使用 `parameters` 来存储更新的参数值，并使用 `scores` 向量来存储MSE值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7872aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.16393857, 0.14458733, 0.14269595, 0.13759389, 0.2210258 ,\n",
      "       0.22809458, 0.21992283, 0.21381054, 0.31796441, 0.30540786,\n",
      "       0.28758755, 0.28683753, 0.46921277, 0.40819283, 0.38299212,\n",
      "       0.38239651]), 'std_fit_time': array([0.02786993, 0.00442478, 0.00139862, 0.00344823, 0.00166341,\n",
      "       0.00940687, 0.01077617, 0.00268722, 0.00797422, 0.01180642,\n",
      "       0.00219321, 0.00875536, 0.01614436, 0.01567394, 0.00478118,\n",
      "       0.00689056]), 'mean_score_time': array([0.00679994, 0.00659471, 0.00630088, 0.00601444, 0.0066052 ,\n",
      "       0.00679531, 0.00709362, 0.00689321, 0.00700293, 0.00700073,\n",
      "       0.00660291, 0.00699949, 0.00539026, 0.00649624, 0.00680823,\n",
      "       0.00709386]), 'std_score_time': array([9.88771065e-04, 3.68965100e-04, 3.90218208e-04, 6.28076695e-04,\n",
      "       5.91979196e-04, 5.10173925e-04, 4.85149821e-04, 3.77683699e-04,\n",
      "       6.36350874e-04, 1.76258262e-05, 4.72872767e-04, 1.11358377e-03,\n",
      "       8.14399249e-04, 8.93066192e-04, 9.77718697e-04, 2.00566540e-04]), 'param_max_depth': masked_array(data=[2, 2, 2, 2, 4, 4, 4, 4, 6, 6, 6, 6, 8, 8, 8, 8],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_child_weight': masked_array(data=[1, 3, 5, 7, 1, 3, 5, 7, 1, 3, 5, 7, 1, 3, 5, 7],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 2, 'min_child_weight': 1}, {'max_depth': 2, 'min_child_weight': 3}, {'max_depth': 2, 'min_child_weight': 5}, {'max_depth': 2, 'min_child_weight': 7}, {'max_depth': 4, 'min_child_weight': 1}, {'max_depth': 4, 'min_child_weight': 3}, {'max_depth': 4, 'min_child_weight': 5}, {'max_depth': 4, 'min_child_weight': 7}, {'max_depth': 6, 'min_child_weight': 1}, {'max_depth': 6, 'min_child_weight': 3}, {'max_depth': 6, 'min_child_weight': 5}, {'max_depth': 6, 'min_child_weight': 7}, {'max_depth': 8, 'min_child_weight': 1}, {'max_depth': 8, 'min_child_weight': 3}, {'max_depth': 8, 'min_child_weight': 5}, {'max_depth': 8, 'min_child_weight': 7}], 'split0_test_score': array([0.90667895, 0.92419232, 0.91606299, 0.92425335, 0.92659135,\n",
      "       0.92442072, 0.90236054, 0.91058411, 0.90729926, 0.92572269,\n",
      "       0.89364368, 0.90854866, 0.90898366, 0.91978814, 0.8995748 ,\n",
      "       0.90493052]), 'split1_test_score': array([0.88901361, 0.90471324, 0.92135005, 0.92527001, 0.91188098,\n",
      "       0.89403545, 0.95165586, 0.94311959, 0.90435086, 0.87751025,\n",
      "       0.95151092, 0.95064176, 0.91064743, 0.86946867, 0.95139436,\n",
      "       0.9484481 ]), 'split2_test_score': array([0.94162642, 0.94003923, 0.93897515, 0.93816132, 0.97836364,\n",
      "       0.97219608, 0.96942392, 0.96933687, 0.97354665, 0.97161151,\n",
      "       0.97220398, 0.97143164, 0.96734647, 0.96736332, 0.97027386,\n",
      "       0.97176136]), 'split3_test_score': array([0.89655647, 0.88661744, 0.88911322, 0.86622552, 0.89129738,\n",
      "       0.8784511 , 0.90119821, 0.89614364, 0.89194688, 0.89909225,\n",
      "       0.9003103 , 0.8948907 , 0.88665203, 0.88923444, 0.89725229,\n",
      "       0.89687163]), 'split4_test_score': array([0.93457782, 0.94805977, 0.93620809, 0.93649404, 0.96678263,\n",
      "       0.96374484, 0.96229376, 0.95762619, 0.96698753, 0.9706938 ,\n",
      "       0.96390023, 0.96538591, 0.96004812, 0.96435239, 0.96501344,\n",
      "       0.96513565]), 'mean_test_score': array([0.91369065, 0.9207244 , 0.9203419 , 0.91808085, 0.93498319,\n",
      "       0.92656964, 0.93738646, 0.93536208, 0.92882624, 0.9289261 ,\n",
      "       0.93631382, 0.93817974, 0.92673554, 0.92204139, 0.93670175,\n",
      "       0.93742945]), 'std_test_score': array([0.0208249 , 0.02259183, 0.01785211, 0.02653688, 0.03288055,\n",
      "       0.03699258, 0.02962024, 0.02779277, 0.03428918, 0.03771073,\n",
      "       0.03285441, 0.03083215, 0.03143126, 0.03921622, 0.03187219,\n",
      "       0.03088314]), 'rank_test_score': array([16, 13, 14, 15,  7, 11,  3,  6,  9,  8,  5,  1, 10, 12,  4,  2])}\n",
      "Best parameters {'max_depth': 6, 'min_child_weight': 7}\n",
      "Best score 0.9381797366329888\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 100\n",
    "learning_rate = 0.1\n",
    "gamma = 0.1\n",
    "subsample = 0.8\n",
    "colsample_bytree = 0.8\n",
    "reg_alpha = 1\n",
    "reg_lambda = 1\n",
    "\n",
    "parameters = {}\n",
    "parameters['n_estimators'] = n_estimators    # 最大的迭代次数,决策树的个数\n",
    "parameters['learning_rate'] = learning_rate    # 学习速率，控制每次迭代更新权重时的步长\n",
    "parameters['gamma'] = gamma    # 惩罚项系数，指定节点分裂所需的最小损失函数下降值\n",
    "parameters['subsample'] = subsample    # 训练每棵树时，使用的数据占全部训练集的比例\n",
    "parameters['colsample_bytree'] = colsample_bytree    # 随机选择N%特征建立决策树\n",
    "parameters['reg_alpha'] = reg_alpha    # L1正则化参数\n",
    "parameters['reg_lambda'] = reg_lambda    # L2正则化参数\n",
    "\n",
    "scores = []\n",
    "\n",
    "cv_params = {'max_depth': [2,4,6,8],\n",
    "             'min_child_weight': [1,3,5,7]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = \"reg:squarederror\",\n",
    "                                        seed = 99,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        gamma = gamma,\n",
    "                                        subsample = subsample,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                    ),\n",
    "                    \n",
    "                    param_grid = cv_params,\n",
    "                    cv = 5,\n",
    ")\n",
    "\n",
    "gbm.fit(X_train,y_train)\n",
    "print(gbm.cv_results_)\n",
    "print(\"Best parameters %s\" %gbm.best_params_)\n",
    "print(\"Best score %s\" %gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766c53f",
   "metadata": {},
   "source": [
    "在上述大网格得到的最佳值基础上，用较小的网格进行细化寻找最优`max_depth`和`min_child_weight`的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa894f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.24322777, 0.24288874, 0.24389944, 0.24348631, 0.24256968,\n",
      "       0.28514342, 0.28409209, 0.28195667, 0.28185863, 0.28375177,\n",
      "       0.33166375, 0.33282537, 0.33282418, 0.33083344, 0.32913327]), 'std_fit_time': array([0.00294819, 0.00466904, 0.00377087, 0.0036707 , 0.00172084,\n",
      "       0.00376866, 0.00387429, 0.00472774, 0.00497921, 0.00355597,\n",
      "       0.00517076, 0.0066606 , 0.0045675 , 0.0036174 , 0.00297651]), 'mean_score_time': array([0.00598969, 0.00660367, 0.00699077, 0.00621448, 0.00669203,\n",
      "       0.00659857, 0.00708728, 0.00658283, 0.00678229, 0.00698156,\n",
      "       0.00698128, 0.00718045, 0.00738034, 0.0065825 , 0.00698118]), 'std_score_time': array([6.33426945e-04, 8.08339343e-04, 1.15770327e-05, 3.93249851e-04,\n",
      "       5.96197178e-04, 4.83378711e-04, 2.00235523e-04, 4.88792316e-04,\n",
      "       3.98755198e-04, 2.78041453e-07, 6.14361702e-07, 3.98993531e-04,\n",
      "       4.88636109e-04, 4.89103591e-04, 4.10190833e-07]), 'param_max_depth': masked_array(data=[5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_min_child_weight': masked_array(data=[6, 6.5, 7, 7.5, 8, 6, 6.5, 7, 7.5, 8, 6, 6.5, 7, 7.5,\n",
      "                   8],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'max_depth': 5, 'min_child_weight': 6}, {'max_depth': 5, 'min_child_weight': 6.5}, {'max_depth': 5, 'min_child_weight': 7}, {'max_depth': 5, 'min_child_weight': 7.5}, {'max_depth': 5, 'min_child_weight': 8}, {'max_depth': 6, 'min_child_weight': 6}, {'max_depth': 6, 'min_child_weight': 6.5}, {'max_depth': 6, 'min_child_weight': 7}, {'max_depth': 6, 'min_child_weight': 7.5}, {'max_depth': 6, 'min_child_weight': 8}, {'max_depth': 7, 'min_child_weight': 6}, {'max_depth': 7, 'min_child_weight': 6.5}, {'max_depth': 7, 'min_child_weight': 7}, {'max_depth': 7, 'min_child_weight': 7.5}, {'max_depth': 7, 'min_child_weight': 8}], 'split0_test_score': array([0.90479934, 0.90699131, 0.90699131, 0.90647987, 0.90647987,\n",
      "       0.89987603, 0.90854866, 0.90854866, 0.90611925, 0.90611925,\n",
      "       0.89880607, 0.91170718, 0.91170718, 0.90985443, 0.90985443]), 'split1_test_score': array([0.95114859, 0.94892886, 0.94892886, 0.94979746, 0.94979746,\n",
      "       0.9495517 , 0.95064176, 0.95064176, 0.95072616, 0.95072616,\n",
      "       0.95091027, 0.95036073, 0.95036073, 0.95115979, 0.95115979]), 'split2_test_score': array([0.97161909, 0.97113306, 0.97113306, 0.96788883, 0.96788883,\n",
      "       0.97203406, 0.97143164, 0.97143164, 0.97054201, 0.97054201,\n",
      "       0.9721256 , 0.97056151, 0.97056151, 0.96977733, 0.96977733]), 'split3_test_score': array([0.90023541, 0.89733682, 0.89733682, 0.89491438, 0.89491438,\n",
      "       0.89770036, 0.8948907 , 0.8948907 , 0.89935948, 0.89935948,\n",
      "       0.89897071, 0.89553357, 0.89553357, 0.89782001, 0.89782001]), 'split4_test_score': array([0.96834357, 0.96524526, 0.96524526, 0.96046593, 0.96046593,\n",
      "       0.96922573, 0.96538591, 0.96538591, 0.96051928, 0.96051928,\n",
      "       0.96377889, 0.96424781, 0.96424781, 0.96289095, 0.96289095]), 'mean_test_score': array([0.9392292 , 0.93792706, 0.93792706, 0.9359093 , 0.9359093 ,\n",
      "       0.93767758, 0.93817974, 0.93817974, 0.93745324, 0.93745324,\n",
      "       0.93691831, 0.93848216, 0.93848216, 0.9383005 , 0.9383005 ]), 'std_test_score': array([0.03080505, 0.03024748, 0.03024748, 0.0295475 , 0.0295475 ,\n",
      "       0.03269184, 0.03083215, 0.03083215, 0.02910681, 0.02910681,\n",
      "       0.03177855, 0.02964974, 0.02964974, 0.02901273, 0.02901273]), 'rank_test_score': array([ 1,  8,  8, 14, 14, 10,  6,  6, 11, 11, 13,  2,  2,  4,  4])}\n",
      "Best parameters {'max_depth': 5, 'min_child_weight': 6}\n",
      "Best score 0.9392292025919275\n"
     ]
    }
   ],
   "source": [
    "max_depth = gbm.best_params_['max_depth']\n",
    "min_child_weight = gbm.best_params_['min_child_weight']\n",
    "parameters['max_depth'] = max_depth\n",
    "parameters['min_child_weight'] = min_child_weight\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'max_depth': [max_depth-1, max_depth, max_depth+1], \n",
    "             'min_child_weight': [min_child_weight-1, min_child_weight-0.5, min_child_weight, min_child_weight+0.5, min_child_weight+1]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = \"reg:squarederror\",\n",
    "                                        seed = 99,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        gamma = gamma,\n",
    "                                        subsample = subsample,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    cv = 5,\n",
    ")\n",
    "\n",
    "gbm.fit(X_train,y_train)\n",
    "print(gbm.cv_results_)\n",
    "print(\"Best parameters %s\" %gbm.best_params_)\n",
    "print(\"Best score %s\" %gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc277a0f",
   "metadata": {},
   "source": [
    "设置`max_depth`和`min_child_weight`的参数\n",
    "\n",
    "调整`gamma`参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3449b703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.24834499, 0.24784975, 0.24874506, 0.24804726, 0.24894376]), 'std_fit_time': array([0.00324498, 0.00314198, 0.00261607, 0.00149705, 0.00079669]), 'mean_score_time': array([0.00698142, 0.00678191, 0.00697913, 0.00698142, 0.00698161]), 'std_score_time': array([4.62310777e-07, 3.98803053e-04, 4.85484863e-06, 5.91739352e-07,\n",
      "       4.76837158e-07]), 'param_gamma': masked_array(data=[0.1, 0.3, 0.5, 0.7, 0.9],\n",
      "             mask=[False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'gamma': 0.1}, {'gamma': 0.3}, {'gamma': 0.5}, {'gamma': 0.7}, {'gamma': 0.9}], 'split0_test_score': array([0.90479934, 0.90479934, 0.90479934, 0.90479934, 0.90479934]), 'split1_test_score': array([0.95114859, 0.95114859, 0.95114859, 0.95114859, 0.95114859]), 'split2_test_score': array([0.97161909, 0.97161909, 0.97161909, 0.97161909, 0.97161909]), 'split3_test_score': array([0.90023541, 0.90023541, 0.90023541, 0.90023541, 0.90023541]), 'split4_test_score': array([0.96834357, 0.96834357, 0.96834357, 0.96834357, 0.96834357]), 'mean_test_score': array([0.9392292, 0.9392292, 0.9392292, 0.9392292, 0.9392292]), 'std_test_score': array([0.03080505, 0.03080505, 0.03080505, 0.03080505, 0.03080505]), 'rank_test_score': array([1, 1, 1, 1, 1])}\n",
      "Best parameters {'gamma': 0.1}\n",
      "Best score 0.9392292025919275\n"
     ]
    }
   ],
   "source": [
    "max_depth = gbm.best_params_['max_depth']\n",
    "min_child_weight = gbm.best_params_['min_child_weight']\n",
    "parameters['max_depth'] = max_depth\n",
    "parameters['min_child_weight'] = min_child_weight\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'gamma': [i/10.0 for i in range(1,10,2)]}\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = \"reg:squarederror\",\n",
    "                                        seed = 99,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        subsample = subsample,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    cv = 5,\n",
    ")\n",
    "\n",
    "gbm.fit(X_train,y_train)\n",
    "print(gbm.cv_results_)\n",
    "print(\"Best parameters %s\" %gbm.best_params_)\n",
    "print(\"Best score %s\" %gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ed0067",
   "metadata": {},
   "source": [
    "设置`gamma`参数\n",
    "\n",
    "调整`learning_rate`和`n_estimators`参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "107e5b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.13384857, 0.24944615, 0.36433611, 0.48252974, 0.59523134,\n",
      "       0.71991029, 0.83898702, 0.95339141, 1.07347383, 0.13115392,\n",
      "       0.25143814, 0.37082186, 0.49121127, 0.61956635, 0.75122452,\n",
      "       0.87302518, 1.00315819, 1.12882948, 0.13424788, 0.25323381,\n",
      "       0.38139839, 0.51624141, 0.6525836 , 0.78722653, 0.91408777,\n",
      "       1.03796701, 1.16643195]), 'std_fit_time': array([0.00182805, 0.00207941, 0.00200947, 0.00446853, 0.00619337,\n",
      "       0.00742856, 0.009814  , 0.00613617, 0.00859039, 0.00341305,\n",
      "       0.00123639, 0.00609329, 0.00443088, 0.0071569 , 0.00763298,\n",
      "       0.00507706, 0.0079527 , 0.01076899, 0.00184622, 0.00115211,\n",
      "       0.00565931, 0.00667901, 0.00852423, 0.00604215, 0.0054642 ,\n",
      "       0.00607353, 0.00965673]), 'mean_score_time': array([0.00698113, 0.00678234, 0.0072824 , 0.00698113, 0.00777874,\n",
      "       0.00757732, 0.00638337, 0.00558553, 0.00598469, 0.00658221,\n",
      "       0.00698137, 0.00678124, 0.00738015, 0.007482  , 0.00638337,\n",
      "       0.00538583, 0.00558619, 0.00578523, 0.00718064, 0.00718126,\n",
      "       0.00717878, 0.00757933, 0.00658274, 0.00538473, 0.00538616,\n",
      "       0.00538616, 0.0059844 ]), 'std_score_time': array([6.30902089e-04, 3.99136839e-04, 3.99795203e-04, 6.30826708e-04,\n",
      "       3.99041443e-04, 4.86131229e-04, 1.01690613e-03, 4.88499820e-04,\n",
      "       6.30751094e-04, 7.97725000e-04, 6.30600349e-04, 3.99541926e-04,\n",
      "       4.88791804e-04, 8.90181599e-04, 1.19647981e-03, 4.88947562e-04,\n",
      "       4.88752863e-04, 3.98874582e-04, 3.98898278e-04, 3.99780643e-04,\n",
      "       4.00336047e-04, 4.88130469e-04, 1.01655982e-03, 4.87218955e-04,\n",
      "       4.88772332e-04, 4.88480452e-04, 6.84390073e-07]), 'param_learning_rate': masked_array(data=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2,\n",
      "                   0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3,\n",
      "                   0.3, 0.3, 0.3, 0.3, 0.3],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_n_estimators': masked_array(data=[50, 100, 150, 200, 250, 300, 350, 400, 450, 50, 100,\n",
      "                   150, 200, 250, 300, 350, 400, 450, 50, 100, 150, 200,\n",
      "                   250, 300, 350, 400, 450],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'learning_rate': 0.1, 'n_estimators': 50}, {'learning_rate': 0.1, 'n_estimators': 100}, {'learning_rate': 0.1, 'n_estimators': 150}, {'learning_rate': 0.1, 'n_estimators': 200}, {'learning_rate': 0.1, 'n_estimators': 250}, {'learning_rate': 0.1, 'n_estimators': 300}, {'learning_rate': 0.1, 'n_estimators': 350}, {'learning_rate': 0.1, 'n_estimators': 400}, {'learning_rate': 0.1, 'n_estimators': 450}, {'learning_rate': 0.2, 'n_estimators': 50}, {'learning_rate': 0.2, 'n_estimators': 100}, {'learning_rate': 0.2, 'n_estimators': 150}, {'learning_rate': 0.2, 'n_estimators': 200}, {'learning_rate': 0.2, 'n_estimators': 250}, {'learning_rate': 0.2, 'n_estimators': 300}, {'learning_rate': 0.2, 'n_estimators': 350}, {'learning_rate': 0.2, 'n_estimators': 400}, {'learning_rate': 0.2, 'n_estimators': 450}, {'learning_rate': 0.3, 'n_estimators': 50}, {'learning_rate': 0.3, 'n_estimators': 100}, {'learning_rate': 0.3, 'n_estimators': 150}, {'learning_rate': 0.3, 'n_estimators': 200}, {'learning_rate': 0.3, 'n_estimators': 250}, {'learning_rate': 0.3, 'n_estimators': 300}, {'learning_rate': 0.3, 'n_estimators': 350}, {'learning_rate': 0.3, 'n_estimators': 400}, {'learning_rate': 0.3, 'n_estimators': 450}], 'split0_test_score': array([0.88254264, 0.90479934, 0.91232569, 0.91562851, 0.92159079,\n",
      "       0.9231143 , 0.9254693 , 0.92749455, 0.92930776, 0.89291456,\n",
      "       0.91236607, 0.9171878 , 0.91653911, 0.91884842, 0.92054923,\n",
      "       0.92232053, 0.92293937, 0.92311643, 0.90880201, 0.92302873,\n",
      "       0.92367243, 0.9269778 , 0.92804201, 0.92835044, 0.92857526,\n",
      "       0.92863292, 0.92864601]), 'split1_test_score': array([0.94773812, 0.95114859, 0.95739965, 0.96069804, 0.96293445,\n",
      "       0.96492613, 0.96572343, 0.96632253, 0.96772754, 0.92133117,\n",
      "       0.93444849, 0.93959095, 0.94159928, 0.94181449, 0.94264664,\n",
      "       0.94331178, 0.94326472, 0.94326655, 0.92825896, 0.93735082,\n",
      "       0.94097867, 0.94163313, 0.94135092, 0.9413394 , 0.94133298,\n",
      "       0.94133582, 0.9413093 ]), 'split2_test_score': array([0.9629205 , 0.97161909, 0.97633067, 0.97875348, 0.97966417,\n",
      "       0.9808407 , 0.98128592, 0.98145506, 0.9817763 , 0.96754195,\n",
      "       0.97410459, 0.97679512, 0.97789811, 0.97867295, 0.97942039,\n",
      "       0.97958741, 0.97967286, 0.97975232, 0.96479148, 0.96974484,\n",
      "       0.97011057, 0.96962787, 0.96962605, 0.96990374, 0.97003414,\n",
      "       0.97004381, 0.97006896]), 'split3_test_score': array([0.87839788, 0.90023541, 0.91283812, 0.91904514, 0.92183924,\n",
      "       0.92490626, 0.92636906, 0.9277911 , 0.92875603, 0.89247826,\n",
      "       0.91362906, 0.92237675, 0.92485349, 0.92611343, 0.92613828,\n",
      "       0.92611179, 0.9261885 , 0.92612834, 0.90354092, 0.91344119,\n",
      "       0.91468012, 0.91494782, 0.91508376, 0.91531233, 0.91516934,\n",
      "       0.91524236, 0.91521431]), 'split4_test_score': array([0.9640282 , 0.96834357, 0.97090225, 0.97314921, 0.97358057,\n",
      "       0.97352167, 0.97371161, 0.97423623, 0.9741179 , 0.96841764,\n",
      "       0.96878675, 0.96680148, 0.96739968, 0.96767003, 0.9679696 ,\n",
      "       0.96824167, 0.96823901, 0.96832487, 0.95802378, 0.95510993,\n",
      "       0.95594034, 0.95657048, 0.95638982, 0.95651872, 0.95649946,\n",
      "       0.95645787, 0.9564872 ]), 'mean_test_score': array([0.92712547, 0.9392292 , 0.94595928, 0.94945488, 0.95192184,\n",
      "       0.95346181, 0.95451187, 0.9554599 , 0.9563371 , 0.92853671,\n",
      "       0.94066699, 0.94455042, 0.94565793, 0.94662387, 0.94734483,\n",
      "       0.94791464, 0.94806089, 0.9481177 , 0.93268343, 0.9397351 ,\n",
      "       0.94107643, 0.94195142, 0.94209851, 0.94228492, 0.94232224,\n",
      "       0.94234255, 0.94234515]), 'std_test_score': array([0.03854863, 0.03080505, 0.02794167, 0.02688945, 0.02523866,\n",
      "       0.02457569, 0.02386071, 0.02321172, 0.02273479, 0.03386138,\n",
      "       0.02637975, 0.02366352, 0.02370374, 0.02317485, 0.02299573,\n",
      "       0.02266258, 0.02253428, 0.02254421, 0.02494893, 0.02055786,\n",
      "       0.02031547, 0.0196632 , 0.01944195, 0.01943185, 0.01947399,\n",
      "       0.01944217, 0.01945985]), 'rank_test_score': array([27, 24, 12,  6,  5,  4,  3,  2,  1, 26, 22, 14, 13, 11, 10,  9,  8,\n",
      "        7, 25, 23, 21, 20, 19, 18, 17, 16, 15])}\n",
      "Best parameters {'learning_rate': 0.1, 'n_estimators': 450}\n",
      "Best score 0.9563371042431406\n"
     ]
    }
   ],
   "source": [
    "gamma = gbm.best_params_['gamma']\n",
    "parameters['gamma'] = gamma\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'learning_rate':[i/10.0 for i in range(1,4)],\n",
    "             'n_estimators':range(50,500,50)\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = \"reg:squarederror\",\n",
    "                                        seed = 99,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        gamma = gamma,\n",
    "                                        subsample = subsample,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    cv = 5,\n",
    ")\n",
    "\n",
    "gbm.fit(X_train,y_train)\n",
    "print(gbm.cv_results_)\n",
    "print(\"Best parameters %s\" %gbm.best_params_)\n",
    "print(\"Best score %s\" %gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b4e43a",
   "metadata": {},
   "source": [
    "设置`learning_rate`和`n_estimators`参数\n",
    "\n",
    "调整`subsample`和`colsample_bytree`参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3c08c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.99918933, 1.04838343, 1.10446897, 1.1554738 , 1.09792194,\n",
      "       1.00750184, 1.05872097, 1.12609596, 1.17990165, 1.12591014,\n",
      "       1.03839188, 1.09759879, 1.15470991, 1.20668712, 1.14290051,\n",
      "       1.05820441, 1.11960273, 1.17440958, 1.24808674, 1.17271671,\n",
      "       1.08403692, 1.15887899, 1.2653903 , 1.27820234, 1.19830313]), 'std_fit_time': array([0.00665783, 0.00440984, 0.00418051, 0.01048494, 0.01853008,\n",
      "       0.00111755, 0.00422974, 0.01046539, 0.01724376, 0.00880773,\n",
      "       0.00356147, 0.00374363, 0.00848141, 0.0093333 , 0.01472441,\n",
      "       0.01003462, 0.00499493, 0.01251596, 0.00697863, 0.00665982,\n",
      "       0.00401904, 0.00609867, 0.03654922, 0.00957309, 0.01417662]), 'mean_score_time': array([0.00529571, 0.00599084, 0.00620279, 0.0060019 , 0.00579786,\n",
      "       0.00599236, 0.00609055, 0.00570707, 0.0058898 , 0.00599847,\n",
      "       0.00599346, 0.00599856, 0.00629468, 0.00629969, 0.00639515,\n",
      "       0.00659161, 0.00600314, 0.0065906 , 0.0068996 , 0.00639019,\n",
      "       0.0063868 , 0.00610166, 0.00749588, 0.00650935, 0.00649428]), 'std_score_time': array([4.07233852e-04, 6.30729844e-04, 4.06055346e-04, 6.33778677e-04,\n",
      "       3.93973843e-04, 1.61678036e-05, 4.99668773e-04, 3.99969331e-04,\n",
      "       4.91596368e-04, 6.20256234e-04, 1.06278007e-05, 1.99026826e-05,\n",
      "       4.05323040e-04, 4.00854183e-04, 5.79680157e-04, 5.75680179e-04,\n",
      "       6.28577569e-04, 7.85420107e-04, 4.83208552e-04, 4.90450942e-04,\n",
      "       3.72787783e-04, 1.94129209e-04, 6.39178236e-04, 4.43996990e-04,\n",
      "       4.44592774e-04]), 'param_colsample_bytree': masked_array(data=[0.6, 0.6, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.7, 0.8,\n",
      "                   0.8, 0.8, 0.8, 0.8, 0.9, 0.9, 0.9, 0.9, 0.9, 1.0, 1.0,\n",
      "                   1.0, 1.0, 1.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_subsample': masked_array(data=[0.6, 0.7, 0.8, 0.9, 1.0, 0.6, 0.7, 0.8, 0.9, 1.0, 0.6,\n",
      "                   0.7, 0.8, 0.9, 1.0, 0.6, 0.7, 0.8, 0.9, 1.0, 0.6, 0.7,\n",
      "                   0.8, 0.9, 1.0],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'colsample_bytree': 0.6, 'subsample': 0.6}, {'colsample_bytree': 0.6, 'subsample': 0.7}, {'colsample_bytree': 0.6, 'subsample': 0.8}, {'colsample_bytree': 0.6, 'subsample': 0.9}, {'colsample_bytree': 0.6, 'subsample': 1.0}, {'colsample_bytree': 0.7, 'subsample': 0.6}, {'colsample_bytree': 0.7, 'subsample': 0.7}, {'colsample_bytree': 0.7, 'subsample': 0.8}, {'colsample_bytree': 0.7, 'subsample': 0.9}, {'colsample_bytree': 0.7, 'subsample': 1.0}, {'colsample_bytree': 0.8, 'subsample': 0.6}, {'colsample_bytree': 0.8, 'subsample': 0.7}, {'colsample_bytree': 0.8, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'subsample': 0.9}, {'colsample_bytree': 0.8, 'subsample': 1.0}, {'colsample_bytree': 0.9, 'subsample': 0.6}, {'colsample_bytree': 0.9, 'subsample': 0.7}, {'colsample_bytree': 0.9, 'subsample': 0.8}, {'colsample_bytree': 0.9, 'subsample': 0.9}, {'colsample_bytree': 0.9, 'subsample': 1.0}, {'colsample_bytree': 1.0, 'subsample': 0.6}, {'colsample_bytree': 1.0, 'subsample': 0.7}, {'colsample_bytree': 1.0, 'subsample': 0.8}, {'colsample_bytree': 1.0, 'subsample': 0.9}, {'colsample_bytree': 1.0, 'subsample': 1.0}], 'split0_test_score': array([0.92007395, 0.92761514, 0.91843893, 0.91962554, 0.92333161,\n",
      "       0.92453844, 0.93378647, 0.92193643, 0.92735602, 0.90515087,\n",
      "       0.93075948, 0.93271932, 0.92930776, 0.93015089, 0.92711024,\n",
      "       0.93605183, 0.92346512, 0.92920633, 0.9288114 , 0.9328302 ,\n",
      "       0.93101686, 0.93192986, 0.94434462, 0.92537293, 0.92885122]), 'split1_test_score': array([0.94325967, 0.96151603, 0.95036537, 0.95419787, 0.91020919,\n",
      "       0.95247896, 0.96375969, 0.96002702, 0.95557271, 0.9207291 ,\n",
      "       0.95809019, 0.9712502 , 0.96772754, 0.95575537, 0.94830871,\n",
      "       0.96305076, 0.97368404, 0.9775807 , 0.95994401, 0.96442411,\n",
      "       0.96578903, 0.97663043, 0.97039217, 0.96952004, 0.96300095]), 'split2_test_score': array([0.97693873, 0.97569424, 0.97340041, 0.9740559 , 0.96607398,\n",
      "       0.97651581, 0.97767923, 0.97670438, 0.97670204, 0.97355478,\n",
      "       0.9810602 , 0.97911966, 0.9817763 , 0.98348381, 0.97902958,\n",
      "       0.98313733, 0.98288532, 0.98655718, 0.98538383, 0.98469844,\n",
      "       0.98640179, 0.98155889, 0.9867654 , 0.9852957 , 0.98458089]), 'split3_test_score': array([0.92959495, 0.92631886, 0.91755325, 0.92239587, 0.90752161,\n",
      "       0.92974428, 0.93146689, 0.92014797, 0.9211273 , 0.91553449,\n",
      "       0.93391824, 0.93649796, 0.92875603, 0.92412688, 0.91636467,\n",
      "       0.93894827, 0.93808314, 0.93344766, 0.92716511, 0.9294622 ,\n",
      "       0.93731202, 0.94109401, 0.92727617, 0.92094368, 0.93040187]), 'split4_test_score': array([0.95152889, 0.95558484, 0.96460612, 0.95436203, 0.95710182,\n",
      "       0.96518625, 0.96171316, 0.97080954, 0.96192299, 0.96499473,\n",
      "       0.9590682 , 0.9706414 , 0.9741179 , 0.96515693, 0.96137143,\n",
      "       0.95548798, 0.97130652, 0.9719577 , 0.96203738, 0.96345147,\n",
      "       0.94911458, 0.96751418, 0.96529075, 0.96166966, 0.96733927]), 'mean_test_score': array([0.94427924, 0.94934582, 0.94487282, 0.94492744, 0.93284764,\n",
      "       0.94969275, 0.95368109, 0.94992507, 0.94853621, 0.9359928 ,\n",
      "       0.95257926, 0.95804571, 0.9563371 , 0.95173478, 0.94643693,\n",
      "       0.95533523, 0.95788483, 0.95974991, 0.95266835, 0.95497328,\n",
      "       0.95392686, 0.95974547, 0.95881382, 0.9525604 , 0.95483484]), 'std_test_score': array([0.01960492, 0.01941001, 0.02314521, 0.0208388 , 0.02423517,\n",
      "       0.01998984, 0.0180625 , 0.02418835, 0.02108023, 0.02776603,\n",
      "       0.0184823 , 0.01940542, 0.02273479, 0.0220562 , 0.02265695,\n",
      "       0.01716329, 0.02294168, 0.02370787, 0.02204806, 0.02090892,\n",
      "       0.02009682, 0.01971225, 0.02079077, 0.02522294, 0.02181739]), 'rank_test_score': array([23, 18, 22, 21, 25, 17, 11, 16, 19, 24, 13,  4,  6, 15, 20,  7,  5,\n",
      "        1, 12,  8, 10,  2,  3, 14,  9])}\n",
      "Best parameters {'colsample_bytree': 0.9, 'subsample': 0.8}\n",
      "Best score 0.9597499128811924\n"
     ]
    }
   ],
   "source": [
    "learning_rate = gbm.best_params_['learning_rate']\n",
    "n_estimators = gbm.best_params_['n_estimators']\n",
    "parameters['learning_rate'] = learning_rate\n",
    "parameters['n_estimators'] = n_estimators\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'subsample': [i/10.0 for i in range(6,11)],\n",
    "             'colsample_bytree': [i/10.0 for i in range(6,11)]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = \"reg:squarederror\",\n",
    "                                        seed = 99,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        gamma = gamma,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    cv = 5,\n",
    ")\n",
    "\n",
    "gbm.fit(X_train,y_train)\n",
    "print(gbm.cv_results_)\n",
    "print(\"Best parameters %s\" %gbm.best_params_)\n",
    "print(\"Best score %s\" %gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ab2a4",
   "metadata": {},
   "source": [
    "在上述大网格得到的最佳值基础上，用较小的网格进行细化寻找最优`subsample`和`colsample_bytree`参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9e172b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([1.19533429, 1.2396996 , 1.27550502, 1.32199225, 1.24360414,\n",
      "       1.27419801, 1.30960283, 1.32500706]), 'std_fit_time': array([0.01501143, 0.00931727, 0.01160489, 0.01300528, 0.00602716,\n",
      "       0.00578182, 0.00524721, 0.00948013]), 'mean_score_time': array([0.00658455, 0.0066956 , 0.00699296, 0.00649495, 0.00680599,\n",
      "       0.00690508, 0.0071043 , 0.00739436]), 'std_score_time': array([0.00049267, 0.00040676, 0.00063893, 0.0004311 , 0.00040273,\n",
      "       0.00048517, 0.00102104, 0.00047758]), 'param_colsample_bytree': masked_array(data=[0.8, 0.8, 0.8, 0.8, 0.85, 0.85, 0.85, 0.85],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_subsample': masked_array(data=[0.7, 0.75, 0.8, 0.85, 0.7, 0.75, 0.8, 0.85],\n",
      "             mask=[False, False, False, False, False, False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'colsample_bytree': 0.8, 'subsample': 0.7}, {'colsample_bytree': 0.8, 'subsample': 0.75}, {'colsample_bytree': 0.8, 'subsample': 0.8}, {'colsample_bytree': 0.8, 'subsample': 0.85}, {'colsample_bytree': 0.85, 'subsample': 0.7}, {'colsample_bytree': 0.85, 'subsample': 0.75}, {'colsample_bytree': 0.85, 'subsample': 0.8}, {'colsample_bytree': 0.85, 'subsample': 0.85}], 'split0_test_score': array([0.93271932, 0.93805854, 0.92930776, 0.92855637, 0.93271932,\n",
      "       0.93805854, 0.92930776, 0.92855637]), 'split1_test_score': array([0.9712502 , 0.96435032, 0.96772754, 0.9681081 , 0.9712502 ,\n",
      "       0.96435032, 0.96772754, 0.9681081 ]), 'split2_test_score': array([0.97911966, 0.98183797, 0.9817763 , 0.98169293, 0.97911966,\n",
      "       0.98183797, 0.9817763 , 0.98169293]), 'split3_test_score': array([0.93649796, 0.93261087, 0.92875603, 0.92744158, 0.93649796,\n",
      "       0.93261087, 0.92875603, 0.92744158]), 'split4_test_score': array([0.9706414 , 0.97208221, 0.9741179 , 0.96213957, 0.9706414 ,\n",
      "       0.97208221, 0.9741179 , 0.96213957]), 'mean_test_score': array([0.95804571, 0.95778798, 0.9563371 , 0.95358771, 0.95804571,\n",
      "       0.95778798, 0.9563371 , 0.95358771]), 'std_test_score': array([0.01940542, 0.01922982, 0.02273479, 0.02183606, 0.01940542,\n",
      "       0.01922982, 0.02273479, 0.02183606]), 'rank_test_score': array([1, 3, 5, 7, 1, 3, 5, 7])}\n",
      "Best parameters {'colsample_bytree': 0.8, 'subsample': 0.7}\n",
      "Best score 0.9580457076105724\n"
     ]
    }
   ],
   "source": [
    "subsample = gbm.best_params_['subsample']\n",
    "colsample_bytree = gbm.best_params_['colsample_bytree']\n",
    "parameters['subsample'] = subsample\n",
    "parameters['colsample_bytree'] = colsample_bytree\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'subsample': [i/100.0 for i in range(int((subsample-0.1)*100.0), min(int((subsample+0.1)*100),105) , 5)],\n",
    "             'colsample_bytree': [i/100.0 for i in range(int((colsample_bytree-0.1)*100.0), min(int((subsample+0.1)*100),105), 5)]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = \"reg:squarederror\",\n",
    "                                        seed = 99,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        gamma = gamma,\n",
    "                                        reg_alpha = reg_alpha,\n",
    "                                        reg_lambda = reg_lambda,\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    cv = 5,\n",
    ")\n",
    "\n",
    "gbm.fit(X_train,y_train)\n",
    "print(gbm.cv_results_)\n",
    "print(\"Best parameters %s\" %gbm.best_params_)\n",
    "print(\"Best score %s\" %gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5910d151",
   "metadata": {},
   "source": [
    "设置`colsample_bytree`和`subsample`参数\n",
    "\n",
    "调整`reg_alpha`和`reg_lambda`参数\n",
    "\n",
    "`reg_alpha`控制L1正则化，`reg_lambda`控制L2正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e834ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([1.24111733, 1.24099469, 1.24569378, 1.24281797, 1.29119663,\n",
      "       1.24240117, 1.23830032, 1.24319906, 1.24030695, 1.28670764,\n",
      "       1.23710165, 1.23730664, 1.24279399, 1.24051342, 1.28749361,\n",
      "       1.23995419, 1.23737335, 1.24009585, 1.23951511, 1.28790283,\n",
      "       1.23780484, 1.23690534, 1.24080071, 1.24060135, 1.29080429]), 'std_fit_time': array([0.00679982, 0.00142028, 0.00525616, 0.00464881, 0.00543061,\n",
      "       0.00413194, 0.00541203, 0.00480586, 0.00376601, 0.01491563,\n",
      "       0.00317033, 0.00788076, 0.00529277, 0.00208486, 0.00602222,\n",
      "       0.00148935, 0.01202256, 0.00719314, 0.00221941, 0.00577309,\n",
      "       0.00430153, 0.0055083 , 0.00242636, 0.00265744, 0.00898312]), 'mean_score_time': array([0.00659771, 0.00660596, 0.0072978 , 0.00699539, 0.0068975 ,\n",
      "       0.0074049 , 0.00670791, 0.00719185, 0.00650377, 0.00699363,\n",
      "       0.00660582, 0.0067081 , 0.00679178, 0.00688891, 0.00658913,\n",
      "       0.00658841, 0.00680432, 0.00699635, 0.00668998, 0.00659676,\n",
      "       0.00700312, 0.00699911, 0.00699935, 0.00619082, 0.00640588]), 'std_score_time': array([4.90275642e-04, 4.92594493e-04, 5.96639213e-04, 8.90419993e-04,\n",
      "       1.98629459e-04, 1.01592046e-03, 3.97648770e-04, 7.57291097e-04,\n",
      "       4.44103674e-04, 6.37494930e-04, 4.74614384e-04, 7.52065707e-04,\n",
      "       4.03385365e-04, 4.91896656e-04, 7.93743260e-04, 4.93253167e-04,\n",
      "       3.97414654e-04, 6.25521800e-04, 4.02679068e-04, 4.79053031e-04,\n",
      "       2.41080681e-05, 5.49362900e-04, 3.15669822e-04, 5.05997484e-04,\n",
      "       4.77280063e-04]), 'param_reg_alpha': masked_array(data=[1e-05, 1e-05, 1e-05, 1e-05, 1e-05, 0.01, 0.01, 0.01,\n",
      "                   0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 1, 1, 1, 1,\n",
      "                   100, 100, 100, 100, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'param_reg_lambda': masked_array(data=[1e-05, 0.01, 0.1, 1, 100, 1e-05, 0.01, 0.1, 1, 100,\n",
      "                   1e-05, 0.01, 0.1, 1, 100, 1e-05, 0.01, 0.1, 1, 100,\n",
      "                   1e-05, 0.01, 0.1, 1, 100],\n",
      "             mask=[False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False, False, False, False, False, False, False, False,\n",
      "                   False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'reg_alpha': 1e-05, 'reg_lambda': 1e-05}, {'reg_alpha': 1e-05, 'reg_lambda': 0.01}, {'reg_alpha': 1e-05, 'reg_lambda': 0.1}, {'reg_alpha': 1e-05, 'reg_lambda': 1}, {'reg_alpha': 1e-05, 'reg_lambda': 100}, {'reg_alpha': 0.01, 'reg_lambda': 1e-05}, {'reg_alpha': 0.01, 'reg_lambda': 0.01}, {'reg_alpha': 0.01, 'reg_lambda': 0.1}, {'reg_alpha': 0.01, 'reg_lambda': 1}, {'reg_alpha': 0.01, 'reg_lambda': 100}, {'reg_alpha': 0.1, 'reg_lambda': 1e-05}, {'reg_alpha': 0.1, 'reg_lambda': 0.01}, {'reg_alpha': 0.1, 'reg_lambda': 0.1}, {'reg_alpha': 0.1, 'reg_lambda': 1}, {'reg_alpha': 0.1, 'reg_lambda': 100}, {'reg_alpha': 1, 'reg_lambda': 1e-05}, {'reg_alpha': 1, 'reg_lambda': 0.01}, {'reg_alpha': 1, 'reg_lambda': 0.1}, {'reg_alpha': 1, 'reg_lambda': 1}, {'reg_alpha': 1, 'reg_lambda': 100}, {'reg_alpha': 100, 'reg_lambda': 1e-05}, {'reg_alpha': 100, 'reg_lambda': 0.01}, {'reg_alpha': 100, 'reg_lambda': 0.1}, {'reg_alpha': 100, 'reg_lambda': 1}, {'reg_alpha': 100, 'reg_lambda': 100}], 'split0_test_score': array([0.93239202, 0.92598169, 0.92610407, 0.93271929, 0.87453669,\n",
      "       0.93239203, 0.92598165, 0.92610409, 0.93271929, 0.87453669,\n",
      "       0.93239201, 0.92598172, 0.92610408, 0.93271934, 0.87453669,\n",
      "       0.93239202, 0.92598164, 0.92610408, 0.93271932, 0.87453668,\n",
      "       0.93261958, 0.92598145, 0.92790001, 0.93271943, 0.87458775]), 'split1_test_score': array([0.97645237, 0.97637893, 0.97059623, 0.97125023, 0.95911886,\n",
      "       0.97645237, 0.97637895, 0.97059625, 0.97125023, 0.95911886,\n",
      "       0.97645234, 0.97637893, 0.97059623, 0.97125024, 0.95911886,\n",
      "       0.97645239, 0.97637893, 0.97059622, 0.9712502 , 0.95911885,\n",
      "       0.9771856 , 0.97637845, 0.97139273, 0.97135873, 0.95911868]), 'split2_test_score': array([0.97729385, 0.9773101 , 0.98018939, 0.97911966, 0.96097086,\n",
      "       0.97729386, 0.9773101 , 0.98018939, 0.97911966, 0.96097086,\n",
      "       0.97729384, 0.97731011, 0.98018938, 0.97911966, 0.96097086,\n",
      "       0.97729386, 0.9773101 , 0.98018937, 0.97911966, 0.96097086,\n",
      "       0.97729374, 0.97723549, 0.98011961, 0.9790708 , 0.96097053]), 'split3_test_score': array([0.94440565, 0.94327712, 0.94209284, 0.93649799, 0.84321358,\n",
      "       0.94440567, 0.94327713, 0.94209285, 0.93649798, 0.84321358,\n",
      "       0.94440563, 0.94327714, 0.94209287, 0.93649798, 0.84321358,\n",
      "       0.94440565, 0.94327707, 0.94209281, 0.93649796, 0.84321358,\n",
      "       0.94456018, 0.9432531 , 0.94160464, 0.93649669, 0.8432131 ]), 'split4_test_score': array([0.9637113 , 0.96623554, 0.96766899, 0.9706414 , 0.95927753,\n",
      "       0.96371129, 0.96623555, 0.96766899, 0.9706414 , 0.95927753,\n",
      "       0.96371133, 0.96623557, 0.967669  , 0.9706414 , 0.95927753,\n",
      "       0.96371131, 0.96623557, 0.96766905, 0.9706414 , 0.95927754,\n",
      "       0.96383952, 0.96557106, 0.96764293, 0.9706412 , 0.95927733]), 'mean_test_score': array([0.95885104, 0.95783668, 0.95733031, 0.95804572, 0.9194235 ,\n",
      "       0.95885104, 0.95783668, 0.95733031, 0.95804571, 0.9194235 ,\n",
      "       0.95885103, 0.95783669, 0.95733031, 0.95804572, 0.9194235 ,\n",
      "       0.95885105, 0.95783666, 0.95733031, 0.95804571, 0.9194235 ,\n",
      "       0.95909973, 0.95768391, 0.95773198, 0.95805737, 0.91943348]), 'std_test_score': array([0.01778941, 0.0201012 , 0.02006325, 0.01940543, 0.05042425,\n",
      "       0.0177894 , 0.02010121, 0.02006324, 0.01940543, 0.05042425,\n",
      "       0.01778941, 0.02010119, 0.02006324, 0.01940542, 0.05042425,\n",
      "       0.01778942, 0.02010122, 0.02006325, 0.01940542, 0.05042425,\n",
      "       0.01785039, 0.02003622, 0.01967784, 0.01940988, 0.05041519]), 'rank_test_score': array([ 4, 12, 20,  8, 24,  3, 13, 17,  9, 23,  5, 11, 18,  7, 22,  2, 14,\n",
      "       19, 10, 25,  1, 16, 15,  6, 21])}\n",
      "Best parameters {'reg_alpha': 100, 'reg_lambda': 1e-05}\n",
      "Best score 0.9590997258289041\n"
     ]
    }
   ],
   "source": [
    "colsample_bytree = gbm.best_params_['colsample_bytree']\n",
    "subsample = gbm.best_params_['subsample']\n",
    "parameters['colsample_bytree'] = colsample_bytree\n",
    "parameters['subsample'] = subsample\n",
    "scores.append(gbm.best_score_)\n",
    "\n",
    "cv_params = {'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100], \n",
    "             'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n",
    "            }\n",
    "\n",
    "gbm = GridSearchCV(xgb.XGBRegressor(\n",
    "                                        objective = \"reg:squarederror\",\n",
    "                                        seed = 99,\n",
    "                                        n_estimators = n_estimators,\n",
    "                                        learning_rate = learning_rate,\n",
    "                                        max_depth = max_depth,\n",
    "                                        min_child_weight = min_child_weight,\n",
    "                                        gamma = gamma,\n",
    "                                        colsample_bytree = colsample_bytree,\n",
    "                                        subsample = subsample,\n",
    "                                    ),\n",
    "                   \n",
    "                    param_grid = cv_params,\n",
    "                    cv = 5,\n",
    ")\n",
    "\n",
    "gbm.fit(X_train,y_train)\n",
    "print(gbm.cv_results_)\n",
    "print(\"Best parameters %s\" %gbm.best_params_)\n",
    "print(\"Best score %s\" %gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970afc6",
   "metadata": {},
   "source": [
    "设置`reg_alpha`和`reg_lambda`参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebde11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_alpha = gbm.best_params_['reg_alpha']\n",
    "reg_lambda = gbm.best_params_['reg_lambda']\n",
    "parameters['reg_alpha'] = reg_alpha\n",
    "parameters['reg_lambda'] = reg_lambda\n",
    "scores.append(gbm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bbf68c",
   "metadata": {},
   "source": [
    "## 打印最终参数和获得的分数\n",
    "\n",
    "确保分数随着每次迭代而增加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19df7694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 450, 'learning_rate': 0.1, 'gamma': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.8, 'reg_alpha': 100, 'reg_lambda': 1e-05, 'max_depth': 5, 'min_child_weight': 6}\n",
      "[0.9381797366329888, 0.9392292025919275, 0.9392292025919275, 0.9563371042431406, 0.9597499128811924, 0.9580457076105724, 0.9590997258289041]\n"
     ]
    }
   ],
   "source": [
    "print(parameters)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a4e94",
   "metadata": {},
   "source": [
    "创建XGBoost的DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b90982c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDMat = xgb.DMatrix(data = X_train, label = y_train)\n",
    "testDMat = xgb.DMatrix(data = X_test, label = y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1b498c",
   "metadata": {},
   "source": [
    "通过交叉验证，找到最好的树\n",
    "\n",
    "\n",
    "降低学习率并设置一个大的num_boost_round超参数以确保收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "712becda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[14:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[14:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[14:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[14:46:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:970214.67500+29903.52022\ttest-rmse:964854.00000+120666.47407\n",
      "[1]\ttrain-rmse:885524.93750+29742.86575\ttest-rmse:884433.77500+114873.41917\n",
      "[2]\ttrain-rmse:810004.22500+28355.88070\ttest-rmse:810980.76250+112604.40422\n",
      "[3]\ttrain-rmse:744147.72500+25203.00085\ttest-rmse:745819.47500+112401.70294\n",
      "[4]\ttrain-rmse:683489.11250+25507.58933\ttest-rmse:685463.08750+113603.17069\n",
      "[5]\ttrain-rmse:628415.28750+26546.48051\ttest-rmse:633788.45625+110116.97885\n",
      "[6]\ttrain-rmse:585298.02500+27044.90019\ttest-rmse:590034.67500+109879.65274\n",
      "[7]\ttrain-rmse:540119.95000+26724.54476\ttest-rmse:550589.36875+111722.04592\n",
      "[8]\ttrain-rmse:501912.79375+22866.99795\ttest-rmse:511528.58125+117253.12518\n",
      "[9]\ttrain-rmse:468297.79375+24976.34305\ttest-rmse:478836.28125+115272.33088\n",
      "[10]\ttrain-rmse:435808.37500+21957.50118\ttest-rmse:451807.56875+118027.21317\n",
      "[11]\ttrain-rmse:409011.20625+21986.55526\ttest-rmse:428668.78125+118625.37562\n",
      "[12]\ttrain-rmse:384481.28125+23347.06157\ttest-rmse:406152.20625+120388.94163\n",
      "[13]\ttrain-rmse:363900.04375+19312.01628\ttest-rmse:390558.02500+122360.04116\n",
      "[14]\ttrain-rmse:345290.49375+17829.44098\ttest-rmse:374922.17500+121197.32912\n",
      "[15]\ttrain-rmse:326442.31875+17878.77946\ttest-rmse:360250.95937+120591.75394\n",
      "[16]\ttrain-rmse:309210.41250+16042.93416\ttest-rmse:346006.47500+120484.45127\n",
      "[17]\ttrain-rmse:294141.43125+14594.84692\ttest-rmse:333762.42188+121883.25493\n",
      "[18]\ttrain-rmse:281443.41563+13137.49992\ttest-rmse:323269.10313+123779.20808\n",
      "[19]\ttrain-rmse:269634.85938+14480.38511\ttest-rmse:312428.62188+121374.14959\n",
      "[20]\ttrain-rmse:259274.98438+15316.96127\ttest-rmse:302837.58437+120459.74798\n",
      "[21]\ttrain-rmse:250380.04688+16486.29062\ttest-rmse:294644.06250+118780.03850\n",
      "[22]\ttrain-rmse:242586.51875+15916.95015\ttest-rmse:288808.90000+117918.06314\n",
      "[23]\ttrain-rmse:234634.59375+16796.05077\ttest-rmse:283669.18125+114200.54339\n",
      "[24]\ttrain-rmse:228575.91875+16444.79330\ttest-rmse:279668.13438+113486.70646\n",
      "[25]\ttrain-rmse:224024.79375+17242.43414\ttest-rmse:276007.17812+110941.99620\n",
      "[26]\ttrain-rmse:217629.01562+16517.26949\ttest-rmse:271849.53437+110912.14045\n",
      "[27]\ttrain-rmse:213199.87812+17585.52766\ttest-rmse:268261.00938+109758.98812\n",
      "[28]\ttrain-rmse:208273.53125+17042.87100\ttest-rmse:266293.00000+107834.44855\n",
      "[29]\ttrain-rmse:204531.81562+17250.42773\ttest-rmse:262394.55000+106518.76718\n",
      "[30]\ttrain-rmse:200328.74063+17306.14366\ttest-rmse:260493.15625+105231.30888\n",
      "[31]\ttrain-rmse:197158.14062+16882.25243\ttest-rmse:258137.16875+105670.48059\n",
      "[32]\ttrain-rmse:193805.34375+16455.41665\ttest-rmse:257065.55000+104262.57244\n",
      "[33]\ttrain-rmse:191005.52813+15609.80350\ttest-rmse:256310.63437+103379.13355\n",
      "[34]\ttrain-rmse:188548.74063+15173.95954\ttest-rmse:257318.19062+101393.32262\n",
      "[35]\ttrain-rmse:185447.14375+15421.85039\ttest-rmse:254176.94062+101216.57806\n",
      "[36]\ttrain-rmse:183229.67188+15431.01398\ttest-rmse:252001.95312+101650.68071\n",
      "[37]\ttrain-rmse:181293.43438+15558.61695\ttest-rmse:251389.64688+100028.68598\n",
      "[38]\ttrain-rmse:178896.20312+15059.68684\ttest-rmse:248028.99063+100641.95480\n",
      "[39]\ttrain-rmse:176451.31875+14940.75831\ttest-rmse:248109.90000+97729.10905\n",
      "[40]\ttrain-rmse:174679.07500+14921.73139\ttest-rmse:246610.80000+97416.76303\n",
      "[41]\ttrain-rmse:171961.23125+14371.85349\ttest-rmse:244909.08125+97680.48373\n",
      "[42]\ttrain-rmse:170479.38125+14525.27741\ttest-rmse:243632.89688+98036.51669\n",
      "[43]\ttrain-rmse:168743.39375+14503.25983\ttest-rmse:241149.26250+98534.05448\n",
      "[44]\ttrain-rmse:166981.43438+14409.91346\ttest-rmse:240810.93750+98522.79148\n",
      "[45]\ttrain-rmse:164785.82812+13951.36971\ttest-rmse:239952.69062+98673.67739\n",
      "[46]\ttrain-rmse:163415.84375+13931.30791\ttest-rmse:239307.99063+98782.40183\n",
      "[47]\ttrain-rmse:162033.79375+14274.14013\ttest-rmse:239701.25000+98398.11875\n",
      "[48]\ttrain-rmse:160876.43125+14103.09904\ttest-rmse:238913.76250+98634.35653\n",
      "[49]\ttrain-rmse:159328.29375+13753.70037\ttest-rmse:237578.14375+98047.33849\n",
      "[50]\ttrain-rmse:157965.85312+13017.91199\ttest-rmse:236935.49063+98281.41347\n",
      "[51]\ttrain-rmse:156900.36875+12988.74819\ttest-rmse:237698.98125+98092.63392\n",
      "[52]\ttrain-rmse:155020.53437+12503.26972\ttest-rmse:236718.61875+96555.03598\n",
      "[53]\ttrain-rmse:153805.70312+12407.80944\ttest-rmse:236998.42188+95940.92803\n",
      "[54]\ttrain-rmse:152857.21094+11915.07787\ttest-rmse:236665.46875+97028.74475\n",
      "[55]\ttrain-rmse:151523.71406+11803.29637\ttest-rmse:236461.19687+96482.03463\n",
      "[56]\ttrain-rmse:150488.46563+11816.49001\ttest-rmse:236729.36875+97217.42265\n",
      "[57]\ttrain-rmse:149337.06250+11772.30141\ttest-rmse:237246.14062+96417.33348\n",
      "[58]\ttrain-rmse:148567.23438+11709.35802\ttest-rmse:236179.92188+96063.19152\n",
      "[59]\ttrain-rmse:147565.17656+12025.69430\ttest-rmse:234431.83125+96552.95714\n",
      "[60]\ttrain-rmse:146158.14375+11880.52624\ttest-rmse:234640.54375+97259.55783\n",
      "[61]\ttrain-rmse:145601.80156+11943.07145\ttest-rmse:235477.88437+96802.43494\n",
      "[62]\ttrain-rmse:144572.03437+11815.24317\ttest-rmse:233750.97812+96276.88271\n",
      "[63]\ttrain-rmse:143835.93594+11849.95390\ttest-rmse:232344.19062+95997.74098\n",
      "[64]\ttrain-rmse:142823.37812+11929.10056\ttest-rmse:231198.08438+95288.50453\n",
      "[65]\ttrain-rmse:141753.60625+11827.76494\ttest-rmse:230749.02813+95213.07589\n",
      "[66]\ttrain-rmse:140573.68281+11974.19295\ttest-rmse:228915.16250+94704.72391\n",
      "[67]\ttrain-rmse:139790.66094+12144.84266\ttest-rmse:229384.91875+94998.22946\n",
      "[68]\ttrain-rmse:138934.46563+12184.74760\ttest-rmse:228154.25937+95448.10167\n",
      "[69]\ttrain-rmse:137898.97500+12295.32804\ttest-rmse:227267.15625+95715.01883\n",
      "[70]\ttrain-rmse:136552.28125+12565.88219\ttest-rmse:227718.80625+95723.00580\n",
      "[71]\ttrain-rmse:135306.86563+12263.67147\ttest-rmse:228424.57187+95983.63952\n",
      "[72]\ttrain-rmse:133962.58906+11926.54119\ttest-rmse:229048.29062+95267.24251\n",
      "[73]\ttrain-rmse:133051.88750+11584.79391\ttest-rmse:229420.46563+94598.31599\n",
      "[74]\ttrain-rmse:131643.24531+11235.64838\ttest-rmse:228703.47812+95028.90254\n",
      "[75]\ttrain-rmse:130626.05781+11092.84360\ttest-rmse:228099.22812+94678.00335\n",
      "[76]\ttrain-rmse:129589.99688+11056.29217\ttest-rmse:227568.96563+94768.16084\n",
      "[77]\ttrain-rmse:128491.25156+11037.18871\ttest-rmse:227159.03437+94822.02463\n",
      "[78]\ttrain-rmse:127488.00000+11010.26909\ttest-rmse:225910.27188+95277.65531\n",
      "[79]\ttrain-rmse:126538.57344+10916.50581\ttest-rmse:225578.81719+94759.62656\n",
      "[80]\ttrain-rmse:125373.48438+11033.63093\ttest-rmse:226024.74375+95114.09727\n",
      "[81]\ttrain-rmse:124262.74688+11083.24692\ttest-rmse:225219.22812+94885.95613\n",
      "[82]\ttrain-rmse:123273.48594+11009.14561\ttest-rmse:225378.05000+93676.46131\n",
      "[83]\ttrain-rmse:122307.64063+10869.61475\ttest-rmse:225326.43125+93361.14102\n",
      "[84]\ttrain-rmse:121172.02188+10553.97529\ttest-rmse:223788.07500+92365.84969\n",
      "[85]\ttrain-rmse:120498.71094+10685.38369\ttest-rmse:222800.61250+92451.22745\n",
      "[86]\ttrain-rmse:119173.02656+10529.50877\ttest-rmse:222900.08438+92913.21643\n",
      "[87]\ttrain-rmse:118457.16719+10240.43417\ttest-rmse:222624.81250+93620.60501\n",
      "[88]\ttrain-rmse:117492.77500+10145.39076\ttest-rmse:221589.12656+93280.73187\n",
      "[89]\ttrain-rmse:116662.63750+10320.75598\ttest-rmse:221050.65313+93209.57495\n",
      "[90]\ttrain-rmse:116084.73750+10397.89014\ttest-rmse:221519.86719+92522.84177\n",
      "[91]\ttrain-rmse:115210.52031+10831.77030\ttest-rmse:221076.51094+92525.24959\n",
      "[92]\ttrain-rmse:114414.83281+10569.18408\ttest-rmse:220949.56094+93553.73385\n",
      "[93]\ttrain-rmse:113593.89531+10280.18060\ttest-rmse:220201.37344+92614.46381\n",
      "[94]\ttrain-rmse:112962.39688+10202.75759\ttest-rmse:219625.17813+91669.86329\n",
      "[95]\ttrain-rmse:111941.79844+9995.50113\ttest-rmse:219602.12344+91159.65193\n",
      "[96]\ttrain-rmse:110968.32500+10018.06875\ttest-rmse:218950.92813+90655.50702\n",
      "[97]\ttrain-rmse:110088.62031+9905.11552\ttest-rmse:218716.88594+90281.29649\n",
      "[98]\ttrain-rmse:109338.91875+9714.81128\ttest-rmse:218084.80156+90165.69395\n",
      "[99]\ttrain-rmse:108501.32344+9622.05475\ttest-rmse:217736.54375+90151.57827\n",
      "[100]\ttrain-rmse:107925.37031+9952.45156\ttest-rmse:216891.33594+90409.48298\n",
      "[101]\ttrain-rmse:107425.93438+10133.98612\ttest-rmse:217730.56719+90155.27628\n",
      "[102]\ttrain-rmse:106586.74375+9959.98957\ttest-rmse:217890.33438+90280.96559\n",
      "[103]\ttrain-rmse:105707.71406+9949.20277\ttest-rmse:216729.12656+89805.98815\n",
      "[104]\ttrain-rmse:104840.77813+9461.45369\ttest-rmse:217537.25469+89873.35748\n",
      "[105]\ttrain-rmse:104459.01406+9385.35780\ttest-rmse:217255.91406+89769.05353\n",
      "[106]\ttrain-rmse:103652.53438+9350.39940\ttest-rmse:216962.03281+88776.14951\n",
      "[107]\ttrain-rmse:103068.68438+9534.49402\ttest-rmse:216420.90625+88891.11448\n",
      "[108]\ttrain-rmse:102557.64688+9517.00709\ttest-rmse:216152.72656+88195.75099\n",
      "[109]\ttrain-rmse:101941.49688+9447.20821\ttest-rmse:216288.18125+88119.09263\n",
      "[110]\ttrain-rmse:101492.44844+9429.79596\ttest-rmse:215961.59844+88410.54394\n",
      "[111]\ttrain-rmse:101102.81563+9643.61843\ttest-rmse:215593.50000+89278.00840\n",
      "[112]\ttrain-rmse:100415.14375+9582.11459\ttest-rmse:215990.29062+89292.84431\n",
      "[113]\ttrain-rmse:99945.74844+9604.11253\ttest-rmse:217055.81406+89224.35179\n",
      "[114]\ttrain-rmse:99470.33281+9796.42208\ttest-rmse:217151.07344+88984.27610\n",
      "[115]\ttrain-rmse:99204.43125+9961.43224\ttest-rmse:216471.61406+89251.65862\n",
      "[116]\ttrain-rmse:98580.73125+10032.94513\ttest-rmse:215956.71094+89083.45210\n",
      "[117]\ttrain-rmse:97914.72813+9792.18336\ttest-rmse:215411.31094+88151.48779\n",
      "[118]\ttrain-rmse:97604.29688+9664.51687\ttest-rmse:215914.39531+88331.59221\n",
      "[119]\ttrain-rmse:97094.99375+9542.25032\ttest-rmse:214777.31406+88287.02925\n",
      "[120]\ttrain-rmse:96462.78750+9474.27642\ttest-rmse:214707.18906+88867.24447\n",
      "[121]\ttrain-rmse:95792.13750+9305.24665\ttest-rmse:214950.48281+88968.51112\n",
      "[122]\ttrain-rmse:95270.64219+9359.43504\ttest-rmse:214992.88906+89310.93057\n",
      "[123]\ttrain-rmse:94687.88750+9252.88266\ttest-rmse:214431.45625+88333.87742\n",
      "[124]\ttrain-rmse:93955.36250+8987.99617\ttest-rmse:214664.05625+87858.77262\n",
      "[125]\ttrain-rmse:93111.47813+8909.83781\ttest-rmse:213521.82500+87578.13536\n",
      "[126]\ttrain-rmse:92531.53438+8784.21711\ttest-rmse:212756.64375+87059.40159\n",
      "[127]\ttrain-rmse:92231.75625+8811.07823\ttest-rmse:212502.06250+87025.20895\n",
      "[128]\ttrain-rmse:91646.30625+8955.59602\ttest-rmse:212128.73125+86999.99842\n",
      "[129]\ttrain-rmse:91006.24219+8693.57012\ttest-rmse:211850.28125+87655.11511\n",
      "[130]\ttrain-rmse:90370.32500+8476.26382\ttest-rmse:211447.01719+88111.92620\n",
      "[131]\ttrain-rmse:89855.56094+8412.79352\ttest-rmse:210762.92969+87659.22939\n",
      "[132]\ttrain-rmse:89232.07813+8216.88256\ttest-rmse:210581.84219+87829.99451\n",
      "[133]\ttrain-rmse:88719.69375+8167.77303\ttest-rmse:210826.22031+87844.94238\n",
      "[134]\ttrain-rmse:88232.38750+8094.98031\ttest-rmse:210165.60938+87825.77226\n",
      "[135]\ttrain-rmse:87756.66094+8032.60648\ttest-rmse:210121.70312+87798.79067\n",
      "[136]\ttrain-rmse:87260.12656+8187.39840\ttest-rmse:210037.52969+87727.35068\n",
      "[137]\ttrain-rmse:86880.15625+8378.63044\ttest-rmse:209436.21875+87745.57116\n",
      "[138]\ttrain-rmse:86401.43750+8203.29993\ttest-rmse:209225.97500+86969.81364\n",
      "[139]\ttrain-rmse:86010.69219+8312.90733\ttest-rmse:208691.59687+87445.93764\n",
      "[140]\ttrain-rmse:85641.80938+8192.66906\ttest-rmse:208961.63437+87221.73772\n",
      "[141]\ttrain-rmse:85312.36719+8278.29725\ttest-rmse:209445.30469+87105.57633\n",
      "[142]\ttrain-rmse:84787.38594+8203.85924\ttest-rmse:208863.72031+86860.41623\n",
      "[143]\ttrain-rmse:84219.39063+8064.31543\ttest-rmse:208206.28125+86380.21180\n",
      "[144]\ttrain-rmse:83721.55781+8096.52833\ttest-rmse:207510.94844+86062.78490\n",
      "[145]\ttrain-rmse:83116.67813+7950.42977\ttest-rmse:207560.57031+86313.21764\n",
      "[146]\ttrain-rmse:82718.05625+7975.63983\ttest-rmse:207552.59531+86420.43383\n",
      "[147]\ttrain-rmse:82176.70469+7807.28471\ttest-rmse:206864.28281+86093.80077\n",
      "[148]\ttrain-rmse:81733.79688+7835.83077\ttest-rmse:206295.76719+86623.99915\n",
      "[149]\ttrain-rmse:81306.95313+7674.52168\ttest-rmse:206456.00469+86720.96025\n",
      "[150]\ttrain-rmse:81098.69375+7861.61526\ttest-rmse:205920.02813+86735.13681\n",
      "[151]\ttrain-rmse:80590.80156+7770.86907\ttest-rmse:205932.20469+87040.28801\n",
      "[152]\ttrain-rmse:80327.11875+7665.02820\ttest-rmse:206250.19219+86820.04647\n",
      "[153]\ttrain-rmse:79825.29062+7644.13379\ttest-rmse:205507.96250+86439.86644\n",
      "[154]\ttrain-rmse:79514.16094+7480.75172\ttest-rmse:205605.96875+85828.04894\n",
      "[155]\ttrain-rmse:78971.87656+7455.57455\ttest-rmse:205656.33438+86338.77558\n",
      "[156]\ttrain-rmse:78661.30313+7503.02655\ttest-rmse:205340.74375+86765.09505\n",
      "[157]\ttrain-rmse:78174.13594+7667.54836\ttest-rmse:204851.18750+86996.04184\n",
      "[158]\ttrain-rmse:77848.32813+7808.56235\ttest-rmse:204287.50469+87369.73087\n",
      "[159]\ttrain-rmse:77462.56250+7696.45680\ttest-rmse:204008.53437+87646.41515\n",
      "[160]\ttrain-rmse:77028.42031+7591.41150\ttest-rmse:203393.95156+87189.41340\n",
      "[161]\ttrain-rmse:76591.41563+7735.49101\ttest-rmse:203783.70312+87026.28849\n",
      "[162]\ttrain-rmse:76171.28125+7535.39027\ttest-rmse:204091.14375+87253.46923\n",
      "[163]\ttrain-rmse:75675.53438+7371.04700\ttest-rmse:203837.72500+87483.20590\n",
      "[164]\ttrain-rmse:75411.20234+7522.89900\ttest-rmse:204070.53906+87583.83056\n",
      "[165]\ttrain-rmse:75161.46016+7375.05896\ttest-rmse:203181.35000+87398.14825\n",
      "[166]\ttrain-rmse:74691.42109+7225.21995\ttest-rmse:202900.64375+86643.42142\n",
      "[167]\ttrain-rmse:74645.32500+7274.03952\ttest-rmse:202940.20313+86899.65846\n",
      "[168]\ttrain-rmse:74155.71562+7179.71522\ttest-rmse:202362.05469+86521.79325\n",
      "[169]\ttrain-rmse:73918.97578+7318.06248\ttest-rmse:202508.35938+86249.90722\n",
      "[170]\ttrain-rmse:73557.09922+7423.09218\ttest-rmse:202632.95781+86179.90654\n",
      "[171]\ttrain-rmse:73072.09297+7368.11854\ttest-rmse:202845.95156+86242.14286\n",
      "[172]\ttrain-rmse:72701.03672+7298.23686\ttest-rmse:202073.05625+85925.95737\n",
      "[173]\ttrain-rmse:72413.91406+7417.33341\ttest-rmse:201940.32656+85634.99119\n",
      "[174]\ttrain-rmse:72197.36016+7481.92043\ttest-rmse:201751.23750+86090.60369\n",
      "[175]\ttrain-rmse:71701.60547+7342.51730\ttest-rmse:201605.52813+86298.75489\n",
      "[176]\ttrain-rmse:71362.71172+7208.56709\ttest-rmse:201607.23125+86401.38814\n",
      "[177]\ttrain-rmse:71180.26250+7029.20236\ttest-rmse:201196.06250+86211.53930\n",
      "[178]\ttrain-rmse:70781.24063+6857.59062\ttest-rmse:200691.49531+86420.67433\n",
      "[179]\ttrain-rmse:70366.09609+6660.41827\ttest-rmse:200545.96250+86769.46967\n",
      "[180]\ttrain-rmse:70161.47500+6738.83004\ttest-rmse:200522.32188+87062.03165\n",
      "[181]\ttrain-rmse:69836.43359+6664.90834\ttest-rmse:200139.68906+87192.77681\n",
      "[182]\ttrain-rmse:69537.66172+6532.71902\ttest-rmse:199999.70625+86955.13384\n",
      "[183]\ttrain-rmse:69086.52969+6308.20464\ttest-rmse:200126.18750+86401.62362\n",
      "[184]\ttrain-rmse:68804.25781+6144.04014\ttest-rmse:199566.76250+86040.14550\n",
      "[185]\ttrain-rmse:68447.06172+6079.28682\ttest-rmse:199268.05938+86201.00254\n",
      "[186]\ttrain-rmse:68045.99922+6037.81017\ttest-rmse:199093.34219+86102.93756\n",
      "[187]\ttrain-rmse:67742.42813+6107.56491\ttest-rmse:198766.32813+86258.21779\n",
      "[188]\ttrain-rmse:67366.15859+5991.33075\ttest-rmse:198602.78750+86029.41709\n",
      "[189]\ttrain-rmse:66923.30937+5772.34869\ttest-rmse:198108.25469+85713.87822\n",
      "[190]\ttrain-rmse:66574.45781+5694.44884\ttest-rmse:197732.96563+85505.75006\n",
      "[191]\ttrain-rmse:66295.82188+5634.77567\ttest-rmse:197795.80625+85485.63355\n",
      "[192]\ttrain-rmse:66026.66172+5508.25907\ttest-rmse:197665.47812+85657.05922\n",
      "[193]\ttrain-rmse:65712.47500+5553.12193\ttest-rmse:197574.18125+85675.26867\n",
      "[194]\ttrain-rmse:65360.73359+5456.35142\ttest-rmse:197682.18125+85728.18304\n",
      "[195]\ttrain-rmse:65082.85391+5680.61864\ttest-rmse:197367.67656+85836.65118\n",
      "[196]\ttrain-rmse:64941.59141+5651.15764\ttest-rmse:197472.77969+85995.11995\n",
      "[197]\ttrain-rmse:64771.79375+5761.98512\ttest-rmse:197241.99375+86148.92470\n",
      "[198]\ttrain-rmse:64564.20234+5873.39891\ttest-rmse:197109.04219+86131.81288\n",
      "[199]\ttrain-rmse:64317.63984+5742.35231\ttest-rmse:196940.05469+86078.00513\n",
      "[200]\ttrain-rmse:63958.85000+5624.65253\ttest-rmse:196617.73125+85667.93524\n",
      "[201]\ttrain-rmse:63592.18750+5494.34976\ttest-rmse:196057.65156+85179.15327\n",
      "[202]\ttrain-rmse:63346.70703+5384.76769\ttest-rmse:195900.69219+85197.24719\n",
      "[203]\ttrain-rmse:63030.34453+5266.66913\ttest-rmse:195484.50469+84982.47250\n",
      "[204]\ttrain-rmse:62785.72344+5146.01008\ttest-rmse:195553.28750+84708.11328\n",
      "[205]\ttrain-rmse:62490.11406+4980.57588\ttest-rmse:195605.47656+84808.10354\n",
      "[206]\ttrain-rmse:62132.86094+4865.19716\ttest-rmse:195675.87031+84422.39555\n",
      "[207]\ttrain-rmse:61849.87109+4797.36202\ttest-rmse:195259.08125+84137.55990\n",
      "[208]\ttrain-rmse:61589.05937+4752.06290\ttest-rmse:194791.60156+83828.28895\n",
      "[209]\ttrain-rmse:61322.93594+4608.71970\ttest-rmse:195278.74219+83925.16682\n",
      "[210]\ttrain-rmse:61131.26953+4626.15165\ttest-rmse:195359.10156+83980.76443\n",
      "[211]\ttrain-rmse:60992.80391+4722.39661\ttest-rmse:195482.05938+84005.35096\n",
      "[212]\ttrain-rmse:60655.40703+4564.19535\ttest-rmse:195496.41719+84280.04146\n",
      "[213]\ttrain-rmse:60342.61406+4515.50732\ttest-rmse:195583.89375+83918.04164\n",
      "[214]\ttrain-rmse:60130.77891+4388.96945\ttest-rmse:195348.19531+84093.01635\n",
      "[215]\ttrain-rmse:59907.16953+4470.68754\ttest-rmse:194991.09063+84285.96144\n",
      "[216]\ttrain-rmse:59757.87578+4538.71666\ttest-rmse:194942.75313+84529.98664\n",
      "[217]\ttrain-rmse:59433.95156+4479.60372\ttest-rmse:194904.10781+84122.53582\n",
      "[218]\ttrain-rmse:59112.40391+4426.68010\ttest-rmse:194831.55781+84179.41767\n",
      "[219]\ttrain-rmse:58901.67813+4377.49270\ttest-rmse:194797.66562+84009.22487\n",
      "[220]\ttrain-rmse:58676.13359+4271.58830\ttest-rmse:194743.70938+84101.59727\n",
      "[221]\ttrain-rmse:58330.44688+4118.05185\ttest-rmse:194408.59219+83904.01789\n",
      "[222]\ttrain-rmse:58121.63672+4050.74405\ttest-rmse:194512.94688+83998.39684\n",
      "[223]\ttrain-rmse:57872.44063+4000.59713\ttest-rmse:194004.93281+83733.04446\n",
      "[224]\ttrain-rmse:57648.03672+4123.34241\ttest-rmse:194018.34375+83894.63333\n",
      "[225]\ttrain-rmse:57395.75156+4024.26557\ttest-rmse:193568.09375+83887.78924\n",
      "[226]\ttrain-rmse:57123.75938+3930.10070\ttest-rmse:193132.45156+83683.28910\n",
      "[227]\ttrain-rmse:56956.83828+3928.19802\ttest-rmse:192648.59687+83543.21427\n",
      "[228]\ttrain-rmse:56728.35547+3838.36181\ttest-rmse:192263.37188+83316.22435\n",
      "[229]\ttrain-rmse:56406.12031+3831.86925\ttest-rmse:192460.09063+83130.45338\n",
      "[230]\ttrain-rmse:56121.00078+3854.03214\ttest-rmse:192436.56719+82986.34132\n",
      "[231]\ttrain-rmse:55891.41797+3882.99091\ttest-rmse:192822.85938+82967.09843\n",
      "[232]\ttrain-rmse:55658.08438+3728.14057\ttest-rmse:192811.91562+82932.20218\n",
      "[233]\ttrain-rmse:55527.01016+3809.44366\ttest-rmse:192876.26406+82986.71221\n",
      "[234]\ttrain-rmse:55418.81094+3855.54556\ttest-rmse:192917.67344+82870.77841\n",
      "[235]\ttrain-rmse:55113.77656+3725.00009\ttest-rmse:192910.75937+82649.06304\n",
      "[236]\ttrain-rmse:54876.64609+3646.54103\ttest-rmse:193205.03281+82721.53314\n",
      "[237]\ttrain-rmse:54699.67031+3632.59485\ttest-rmse:193379.91094+82736.02895\n",
      "[238]\ttrain-rmse:54387.52969+3669.79481\ttest-rmse:193152.82969+82889.59873\n",
      "[239]\ttrain-rmse:54209.97656+3596.93229\ttest-rmse:193011.73125+83049.42463\n",
      "[240]\ttrain-rmse:54033.37578+3634.66795\ttest-rmse:192681.49063+82825.04637\n",
      "[241]\ttrain-rmse:53736.72500+3497.77938\ttest-rmse:192303.45156+82475.39903\n",
      "[242]\ttrain-rmse:53529.71641+3407.76633\ttest-rmse:191857.67813+82209.12744\n",
      "[243]\ttrain-rmse:53357.37031+3388.46571\ttest-rmse:191900.45469+81935.02815\n",
      "[244]\ttrain-rmse:53108.02578+3314.70103\ttest-rmse:191492.38750+81675.07798\n",
      "[245]\ttrain-rmse:52998.11406+3362.14050\ttest-rmse:191353.55000+81873.32769\n",
      "[246]\ttrain-rmse:52800.40391+3288.30511\ttest-rmse:190858.31562+81676.79791\n",
      "[247]\ttrain-rmse:52636.65469+3426.43921\ttest-rmse:190978.65937+81935.36801\n",
      "[248]\ttrain-rmse:52464.07656+3319.63999\ttest-rmse:191063.26250+82067.92449\n",
      "[249]\ttrain-rmse:52239.30391+3286.22295\ttest-rmse:190957.09219+81693.84164\n",
      "[250]\ttrain-rmse:52064.16953+3328.35768\ttest-rmse:190773.89375+81713.12160\n",
      "[251]\ttrain-rmse:51966.24922+3324.57392\ttest-rmse:190674.90625+81843.44952\n",
      "[252]\ttrain-rmse:51780.37422+3304.54487\ttest-rmse:190072.98906+81706.26756\n",
      "[253]\ttrain-rmse:51500.23438+3248.07467\ttest-rmse:189943.27656+81907.83660\n",
      "[254]\ttrain-rmse:51285.35469+3190.78695\ttest-rmse:190153.06719+81968.08920\n",
      "[255]\ttrain-rmse:51036.51797+3201.55197\ttest-rmse:189848.75937+81678.65874\n",
      "[256]\ttrain-rmse:50869.00313+3149.80740\ttest-rmse:190033.08906+81673.74738\n",
      "[257]\ttrain-rmse:50697.99922+3208.61829\ttest-rmse:190204.93906+81623.79126\n",
      "[258]\ttrain-rmse:50521.07578+3172.23685\ttest-rmse:190036.42344+81612.44634\n",
      "[259]\ttrain-rmse:50284.22266+3161.10243\ttest-rmse:189992.69844+81395.30134\n",
      "[260]\ttrain-rmse:50042.37813+3132.50878\ttest-rmse:189998.19688+81503.53747\n",
      "[261]\ttrain-rmse:49822.37734+3113.61690\ttest-rmse:189996.92031+81136.75267\n",
      "[262]\ttrain-rmse:49644.88047+3057.64478\ttest-rmse:189663.20625+80950.65226\n",
      "[263]\ttrain-rmse:49466.22578+3104.52867\ttest-rmse:189684.29375+80765.77644\n",
      "[264]\ttrain-rmse:49335.40000+3122.10102\ttest-rmse:189876.11875+80803.53515\n",
      "[265]\ttrain-rmse:49065.08828+3031.00370\ttest-rmse:189737.88125+80809.00346\n",
      "[266]\ttrain-rmse:48935.06406+3041.23208\ttest-rmse:189625.07344+80748.58319\n",
      "[267]\ttrain-rmse:48819.75625+3026.17099\ttest-rmse:189651.86094+80903.11747\n",
      "[268]\ttrain-rmse:48682.84688+3097.06791\ttest-rmse:189677.82656+80912.30057\n",
      "[269]\ttrain-rmse:48471.00391+3067.20136\ttest-rmse:189459.70000+80640.30383\n",
      "[270]\ttrain-rmse:48340.27500+3099.78212\ttest-rmse:189462.31562+80660.90578\n",
      "[271]\ttrain-rmse:48215.21953+3031.71326\ttest-rmse:189487.91562+80453.63163\n",
      "[272]\ttrain-rmse:48080.44453+3044.84027\ttest-rmse:189509.05156+80579.46373\n",
      "[273]\ttrain-rmse:47883.24609+2989.84478\ttest-rmse:189355.83750+80455.32028\n",
      "[274]\ttrain-rmse:47663.98125+2959.17479\ttest-rmse:189290.89063+80590.22526\n",
      "[275]\ttrain-rmse:47498.05234+2876.41256\ttest-rmse:189380.73906+80797.53299\n",
      "[276]\ttrain-rmse:47295.45391+2836.13360\ttest-rmse:189050.83594+81052.76697\n",
      "[277]\ttrain-rmse:47069.85547+2773.19906\ttest-rmse:188866.54375+81049.37644\n",
      "[278]\ttrain-rmse:46910.64375+2726.68672\ttest-rmse:188838.77344+81097.76513\n",
      "[279]\ttrain-rmse:46677.74766+2640.28974\ttest-rmse:188897.00156+81103.87632\n",
      "[280]\ttrain-rmse:46551.79531+2658.50091\ttest-rmse:188850.22500+81083.78594\n",
      "[281]\ttrain-rmse:46450.40313+2614.96082\ttest-rmse:188735.72500+80979.73642\n",
      "[282]\ttrain-rmse:46233.24844+2571.53986\ttest-rmse:188770.97344+80701.92998\n",
      "[283]\ttrain-rmse:46053.71563+2560.71118\ttest-rmse:188636.53750+80764.79174\n",
      "[284]\ttrain-rmse:45897.35781+2574.62149\ttest-rmse:188495.09375+80548.47829\n",
      "[285]\ttrain-rmse:45650.28047+2492.50073\ttest-rmse:188201.05625+80541.46832\n",
      "[286]\ttrain-rmse:45560.78125+2529.80450\ttest-rmse:188217.31562+80661.85642\n",
      "[287]\ttrain-rmse:45484.36953+2547.96044\ttest-rmse:188331.77344+80640.94972\n",
      "[288]\ttrain-rmse:45408.14453+2567.72409\ttest-rmse:188215.80469+80850.58453\n",
      "[289]\ttrain-rmse:45251.63828+2579.22565\ttest-rmse:188100.07344+80675.44702\n",
      "[290]\ttrain-rmse:45085.08125+2550.35510\ttest-rmse:188015.62500+80457.37293\n",
      "[291]\ttrain-rmse:44997.58516+2533.42982\ttest-rmse:187806.47656+80291.51337\n",
      "[292]\ttrain-rmse:44846.49766+2536.95639\ttest-rmse:187873.30469+80180.93415\n",
      "[293]\ttrain-rmse:44616.56094+2472.74902\ttest-rmse:187554.22187+79978.74515\n",
      "[294]\ttrain-rmse:44484.55625+2429.75431\ttest-rmse:187389.99219+79769.90732\n",
      "[295]\ttrain-rmse:44330.64453+2452.64318\ttest-rmse:187257.32031+79865.23147\n",
      "[296]\ttrain-rmse:44231.88672+2441.97931\ttest-rmse:187306.66719+79719.24570\n",
      "[297]\ttrain-rmse:44080.41172+2424.94527\ttest-rmse:187334.88125+79722.30481\n",
      "[298]\ttrain-rmse:43900.43750+2412.54332\ttest-rmse:187389.98281+79507.22531\n",
      "[299]\ttrain-rmse:43780.82734+2429.54647\ttest-rmse:187321.47344+79525.58689\n",
      "[300]\ttrain-rmse:43681.57813+2376.96945\ttest-rmse:187393.40156+79685.15034\n",
      "[301]\ttrain-rmse:43592.45000+2380.48696\ttest-rmse:187105.97500+79509.48889\n",
      "[302]\ttrain-rmse:43418.27969+2359.24848\ttest-rmse:187254.43125+79360.26112\n",
      "[303]\ttrain-rmse:43248.41641+2312.88938\ttest-rmse:187224.22812+79333.93129\n",
      "[304]\ttrain-rmse:43128.16406+2292.86778\ttest-rmse:187089.97656+79014.51068\n",
      "[305]\ttrain-rmse:42983.40625+2247.52890\ttest-rmse:186954.48750+79131.73156\n",
      "[306]\ttrain-rmse:42814.70781+2266.77038\ttest-rmse:186893.95625+78918.09088\n",
      "[307]\ttrain-rmse:42742.55078+2259.53577\ttest-rmse:187040.01562+78889.67956\n",
      "[308]\ttrain-rmse:42593.61563+2228.32916\ttest-rmse:186867.77188+78916.97353\n",
      "[309]\ttrain-rmse:42494.80469+2229.41021\ttest-rmse:186679.32500+79023.19391\n",
      "[310]\ttrain-rmse:42392.34609+2218.04346\ttest-rmse:186605.00156+79122.63236\n",
      "[311]\ttrain-rmse:42318.80078+2240.24982\ttest-rmse:186418.00156+78935.77110\n",
      "[312]\ttrain-rmse:42187.21719+2275.65613\ttest-rmse:186631.89375+78786.47282\n",
      "[313]\ttrain-rmse:42080.60156+2279.23793\ttest-rmse:186687.60781+78867.98733\n",
      "[314]\ttrain-rmse:41906.61094+2244.54927\ttest-rmse:186484.27500+78780.56472\n",
      "[315]\ttrain-rmse:41773.30859+2219.66219\ttest-rmse:186379.47500+78573.20305\n",
      "[316]\ttrain-rmse:41598.11250+2193.93387\ttest-rmse:186333.41094+78662.81741\n",
      "[317]\ttrain-rmse:41458.84766+2154.69077\ttest-rmse:186527.74844+78620.20375\n",
      "[318]\ttrain-rmse:41330.33594+2174.68590\ttest-rmse:186501.46875+78681.59487\n",
      "[319]\ttrain-rmse:41159.92578+2147.33869\ttest-rmse:186434.82813+78763.37688\n",
      "[320]\ttrain-rmse:40993.27500+2117.02904\ttest-rmse:186569.75469+78580.75330\n",
      "[321]\ttrain-rmse:40916.63750+2124.38753\ttest-rmse:186515.13594+78473.21051\n",
      "[322]\ttrain-rmse:40807.60547+2134.63207\ttest-rmse:186525.29375+78608.50385\n",
      "[323]\ttrain-rmse:40668.05156+2118.83077\ttest-rmse:186510.21563+78773.72413\n",
      "[324]\ttrain-rmse:40484.04375+2095.16255\ttest-rmse:186190.89062+78621.07975\n",
      "[325]\ttrain-rmse:40384.37266+2117.77473\ttest-rmse:186324.27500+78774.17969\n",
      "[326]\ttrain-rmse:40210.76797+2105.79366\ttest-rmse:186419.27188+78757.20259\n",
      "[327]\ttrain-rmse:40103.34766+2124.73221\ttest-rmse:186281.28281+78742.14922\n",
      "[328]\ttrain-rmse:39968.84844+2104.59905\ttest-rmse:186177.72656+78866.76377\n",
      "[329]\ttrain-rmse:39849.66328+2082.18800\ttest-rmse:186186.04844+78834.30937\n",
      "[330]\ttrain-rmse:39685.26406+2066.71008\ttest-rmse:186117.06719+78863.37161\n",
      "[331]\ttrain-rmse:39564.67266+2053.31339\ttest-rmse:186172.99531+78763.36529\n",
      "[332]\ttrain-rmse:39449.89531+2073.42642\ttest-rmse:186082.39844+78763.92521\n",
      "[333]\ttrain-rmse:39320.39844+2059.63113\ttest-rmse:185684.89688+78467.86101\n",
      "[334]\ttrain-rmse:39194.73047+2053.65952\ttest-rmse:185760.87656+78523.35792\n",
      "[335]\ttrain-rmse:39082.95937+2047.26844\ttest-rmse:185858.29531+78569.62488\n",
      "[336]\ttrain-rmse:38999.80547+2061.13263\ttest-rmse:185689.54375+78557.26734\n",
      "[337]\ttrain-rmse:38869.47969+2050.18885\ttest-rmse:185683.58750+78380.04516\n",
      "[338]\ttrain-rmse:38749.04688+2081.75838\ttest-rmse:185737.26563+78467.17461\n",
      "[339]\ttrain-rmse:38618.53125+2083.28806\ttest-rmse:185703.15313+78305.60468\n",
      "[340]\ttrain-rmse:38506.64844+2099.60346\ttest-rmse:185778.38281+78331.29818\n",
      "[341]\ttrain-rmse:38429.13828+2130.80478\ttest-rmse:185552.57187+78278.99639\n",
      "[342]\ttrain-rmse:38336.05312+2145.80367\ttest-rmse:185338.27188+78209.39095\n",
      "[343]\ttrain-rmse:38255.77500+2148.36336\ttest-rmse:185446.52656+78261.13065\n",
      "[344]\ttrain-rmse:38167.60781+2124.01468\ttest-rmse:185485.44375+78096.07632\n",
      "[345]\ttrain-rmse:38096.38594+2115.14377\ttest-rmse:185521.78437+78125.66584\n",
      "[346]\ttrain-rmse:37978.15000+2089.29901\ttest-rmse:185458.85000+78078.03110\n",
      "[347]\ttrain-rmse:37864.09766+2118.60004\ttest-rmse:185379.82813+78101.10154\n",
      "[348]\ttrain-rmse:37741.14062+2124.45789\ttest-rmse:185430.60156+78026.93166\n",
      "[349]\ttrain-rmse:37570.12422+2084.16403\ttest-rmse:185146.12656+78082.15746\n",
      "[350]\ttrain-rmse:37453.03438+2061.76418\ttest-rmse:185089.70938+77951.98879\n",
      "[351]\ttrain-rmse:37337.36484+2062.36106\ttest-rmse:185181.92500+78061.83197\n",
      "[352]\ttrain-rmse:37213.58672+2028.87044\ttest-rmse:185195.48125+78090.68592\n",
      "[353]\ttrain-rmse:37100.81250+2012.01717\ttest-rmse:185194.07500+78101.52061\n",
      "[354]\ttrain-rmse:37017.99453+2030.42220\ttest-rmse:185223.90625+78086.17484\n",
      "[355]\ttrain-rmse:36884.63750+2006.73365\ttest-rmse:185214.45000+77904.70862\n",
      "[356]\ttrain-rmse:36755.54063+2058.73907\ttest-rmse:185253.44375+77848.55472\n",
      "[357]\ttrain-rmse:36683.44766+2035.32139\ttest-rmse:185086.75156+77729.24627\n",
      "[358]\ttrain-rmse:36551.46797+2003.11293\ttest-rmse:185146.13281+77809.84765\n",
      "[359]\ttrain-rmse:36411.44141+1990.26494\ttest-rmse:185104.57812+77927.33656\n",
      "[360]\ttrain-rmse:36331.03516+1993.95344\ttest-rmse:185138.96094+77899.89909\n",
      "[361]\ttrain-rmse:36217.51797+1976.38203\ttest-rmse:185070.23125+78077.11829\n",
      "[362]\ttrain-rmse:36101.15586+2018.45752\ttest-rmse:184812.91563+77991.45556\n",
      "[363]\ttrain-rmse:36009.27305+1985.27298\ttest-rmse:184833.84219+78031.14621\n",
      "[364]\ttrain-rmse:35911.89531+1977.40699\ttest-rmse:184878.62812+77931.35463\n",
      "[365]\ttrain-rmse:35796.37070+1946.88953\ttest-rmse:184637.44375+77846.10602\n",
      "[366]\ttrain-rmse:35631.33398+1893.67525\ttest-rmse:184619.64062+77750.46178\n",
      "[367]\ttrain-rmse:35547.79375+1884.56245\ttest-rmse:184614.83906+77723.84604\n",
      "[368]\ttrain-rmse:35439.01133+1879.00035\ttest-rmse:184564.64531+77741.73528\n",
      "[369]\ttrain-rmse:35362.39102+1888.46866\ttest-rmse:184488.04062+77783.95219\n",
      "[370]\ttrain-rmse:35240.31250+1934.97958\ttest-rmse:184350.39063+77743.81682\n",
      "[371]\ttrain-rmse:35134.68281+1909.71470\ttest-rmse:184329.10938+77829.86472\n",
      "[372]\ttrain-rmse:35053.06953+1886.14018\ttest-rmse:184298.53125+77855.55283\n",
      "[373]\ttrain-rmse:34932.42813+1841.50348\ttest-rmse:184326.21875+77878.44235\n",
      "[374]\ttrain-rmse:34790.04023+1774.26600\ttest-rmse:184372.15469+77769.35042\n",
      "[375]\ttrain-rmse:34740.40898+1758.23680\ttest-rmse:184433.90781+77762.19271\n",
      "[376]\ttrain-rmse:34647.79492+1773.70137\ttest-rmse:184440.06406+77726.85738\n",
      "[377]\ttrain-rmse:34510.45078+1741.49868\ttest-rmse:184323.17656+77727.60393\n",
      "[378]\ttrain-rmse:34403.80039+1731.70229\ttest-rmse:184393.04531+77756.09760\n",
      "[379]\ttrain-rmse:34276.33008+1700.53870\ttest-rmse:184442.84687+77817.65577\n",
      "[380]\ttrain-rmse:34189.57617+1696.06336\ttest-rmse:184387.38437+77863.87010\n",
      "[381]\ttrain-rmse:34117.31250+1698.83495\ttest-rmse:184336.23750+77814.60979\n",
      "[382]\ttrain-rmse:34033.77969+1678.87154\ttest-rmse:184496.70469+77884.53085\n",
      "[383]\ttrain-rmse:33961.14336+1699.61726\ttest-rmse:184538.00312+77798.02747\n",
      "[384]\ttrain-rmse:33867.53750+1682.60265\ttest-rmse:184648.74844+77869.14089\n",
      "[385]\ttrain-rmse:33765.35781+1693.17574\ttest-rmse:184574.90469+77801.27965\n",
      "[386]\ttrain-rmse:33721.14297+1699.99579\ttest-rmse:184613.72344+77862.89473\n",
      "[387]\ttrain-rmse:33651.04375+1697.94763\ttest-rmse:184672.15000+77903.45693\n",
      "[388]\ttrain-rmse:33562.35313+1717.02344\ttest-rmse:184597.72656+77948.00816\n",
      "[389]\ttrain-rmse:33465.68320+1697.36662\ttest-rmse:184549.94062+77823.07485\n",
      "[390]\ttrain-rmse:33383.65703+1697.33940\ttest-rmse:184632.15000+77764.02765\n",
      "[391]\ttrain-rmse:33300.97617+1696.28460\ttest-rmse:184587.95000+77706.65989\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "parameters['eta'] = learning_rate\n",
    "\n",
    "num_boost_round = 3000\n",
    "early_stopping_rounds = 20\n",
    "\n",
    "xgbCV = xgb.cv(\n",
    "    params = parameters, \n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    nfold = 5,\n",
    "    metrics = {'rmse'},\n",
    "    early_stopping_rounds = early_stopping_rounds,\n",
    "    verbose_eval = True,\n",
    "    seed = 99 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3082029",
   "metadata": {},
   "source": [
    "确定最终XGBoost模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "28bccd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:46:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:576: \n",
      "Parameters: { \"n_estimators\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-rmse:965538.50000\teval-rmse:1017365.62500\n",
      "[1]\ttrain-rmse:884559.00000\teval-rmse:934386.56250\n",
      "[2]\ttrain-rmse:806989.81250\teval-rmse:856963.68750\n",
      "[3]\ttrain-rmse:734917.62500\teval-rmse:780249.00000\n",
      "[4]\ttrain-rmse:671201.25000\teval-rmse:716673.06250\n",
      "[5]\ttrain-rmse:618881.56250\teval-rmse:660595.75000\n",
      "[6]\ttrain-rmse:574810.81250\teval-rmse:616440.50000\n",
      "[7]\ttrain-rmse:538936.00000\teval-rmse:587578.87500\n",
      "[8]\ttrain-rmse:507511.40625\teval-rmse:558231.31250\n",
      "[9]\ttrain-rmse:466293.78125\teval-rmse:522184.31250\n",
      "[10]\ttrain-rmse:431746.00000\teval-rmse:490631.37500\n",
      "[11]\ttrain-rmse:406126.25000\teval-rmse:464516.71875\n",
      "[12]\ttrain-rmse:381394.40625\teval-rmse:438993.40625\n",
      "[13]\ttrain-rmse:361212.90625\teval-rmse:419542.87500\n",
      "[14]\ttrain-rmse:337033.96875\teval-rmse:399208.00000\n",
      "[15]\ttrain-rmse:316637.87500\teval-rmse:378924.65625\n",
      "[16]\ttrain-rmse:296343.62500\teval-rmse:364305.25000\n",
      "[17]\ttrain-rmse:284976.53125\teval-rmse:352728.90625\n",
      "[18]\ttrain-rmse:275928.75000\teval-rmse:344875.03125\n",
      "[19]\ttrain-rmse:263869.43750\teval-rmse:332981.56250\n",
      "[20]\ttrain-rmse:252094.14062\teval-rmse:323407.31250\n",
      "[21]\ttrain-rmse:241347.12500\teval-rmse:315493.65625\n",
      "[22]\ttrain-rmse:232445.82812\teval-rmse:305546.75000\n",
      "[23]\ttrain-rmse:222966.92188\teval-rmse:300122.28125\n",
      "[24]\ttrain-rmse:214861.85938\teval-rmse:296241.21875\n",
      "[25]\ttrain-rmse:209339.57812\teval-rmse:290286.28125\n",
      "[26]\ttrain-rmse:203079.62500\teval-rmse:284568.31250\n",
      "[27]\ttrain-rmse:199810.93750\teval-rmse:283047.25000\n",
      "[28]\ttrain-rmse:193730.56250\teval-rmse:277559.25000\n",
      "[29]\ttrain-rmse:189213.25000\teval-rmse:273602.03125\n",
      "[30]\ttrain-rmse:182518.50000\teval-rmse:271916.00000\n",
      "[31]\ttrain-rmse:179243.68750\teval-rmse:267781.84375\n",
      "[32]\ttrain-rmse:178438.46875\teval-rmse:267507.87500\n",
      "[33]\ttrain-rmse:176476.73438\teval-rmse:267256.43750\n",
      "[34]\ttrain-rmse:174683.01562\teval-rmse:266073.53125\n",
      "[35]\ttrain-rmse:170687.29688\teval-rmse:262355.50000\n",
      "[36]\ttrain-rmse:167886.82812\teval-rmse:260267.67188\n",
      "[37]\ttrain-rmse:165581.81250\teval-rmse:255811.68750\n",
      "[38]\ttrain-rmse:164895.71875\teval-rmse:256371.10938\n",
      "[39]\ttrain-rmse:162797.25000\teval-rmse:254470.46875\n",
      "[40]\ttrain-rmse:161348.92188\teval-rmse:252722.50000\n",
      "[41]\ttrain-rmse:161435.15625\teval-rmse:253433.29688\n",
      "[42]\ttrain-rmse:159104.48438\teval-rmse:251297.12500\n",
      "[43]\ttrain-rmse:158386.25000\teval-rmse:251109.81250\n",
      "[44]\ttrain-rmse:156766.93750\teval-rmse:247516.28125\n",
      "[45]\ttrain-rmse:154982.51562\teval-rmse:247873.98438\n",
      "[46]\ttrain-rmse:152991.65625\teval-rmse:244724.78125\n",
      "[47]\ttrain-rmse:151504.57812\teval-rmse:241636.43750\n",
      "[48]\ttrain-rmse:150511.85938\teval-rmse:240870.06250\n",
      "[49]\ttrain-rmse:150342.15625\teval-rmse:241030.64062\n",
      "[50]\ttrain-rmse:147977.10938\teval-rmse:239157.56250\n",
      "[51]\ttrain-rmse:146096.09375\teval-rmse:238128.00000\n",
      "[52]\ttrain-rmse:145399.43750\teval-rmse:238878.90625\n",
      "[53]\ttrain-rmse:143218.09375\teval-rmse:237344.56250\n",
      "[54]\ttrain-rmse:143031.34375\teval-rmse:237158.14062\n",
      "[55]\ttrain-rmse:141101.90625\teval-rmse:233755.90625\n",
      "[56]\ttrain-rmse:140020.62500\teval-rmse:232334.07812\n",
      "[57]\ttrain-rmse:139220.64062\teval-rmse:232778.96875\n",
      "[58]\ttrain-rmse:138171.67188\teval-rmse:231201.40625\n",
      "[59]\ttrain-rmse:136463.65625\teval-rmse:229549.70312\n",
      "[60]\ttrain-rmse:135651.59375\teval-rmse:229023.78125\n",
      "[61]\ttrain-rmse:134214.84375\teval-rmse:226627.76562\n",
      "[62]\ttrain-rmse:132572.54688\teval-rmse:223946.67188\n",
      "[63]\ttrain-rmse:131737.70312\teval-rmse:223654.00000\n",
      "[64]\ttrain-rmse:130892.78125\teval-rmse:223721.96875\n",
      "[65]\ttrain-rmse:130495.99219\teval-rmse:223795.53125\n",
      "[66]\ttrain-rmse:128920.14844\teval-rmse:222332.14062\n",
      "[67]\ttrain-rmse:127098.68750\teval-rmse:222158.45312\n",
      "[68]\ttrain-rmse:126967.29688\teval-rmse:222464.95312\n",
      "[69]\ttrain-rmse:125635.75000\teval-rmse:220896.46875\n",
      "[70]\ttrain-rmse:123967.39844\teval-rmse:218318.59375\n",
      "[71]\ttrain-rmse:122859.97656\teval-rmse:215346.32812\n",
      "[72]\ttrain-rmse:122160.97656\teval-rmse:214736.35938\n",
      "[73]\ttrain-rmse:120751.30469\teval-rmse:214113.56250\n",
      "[74]\ttrain-rmse:120369.85938\teval-rmse:214257.01562\n",
      "[75]\ttrain-rmse:119466.89844\teval-rmse:213329.98438\n",
      "[76]\ttrain-rmse:118051.39062\teval-rmse:212811.09375\n",
      "[77]\ttrain-rmse:117964.42188\teval-rmse:212999.59375\n",
      "[78]\ttrain-rmse:116869.00000\teval-rmse:212887.60938\n",
      "[79]\ttrain-rmse:116264.50000\teval-rmse:211225.81250\n",
      "[80]\ttrain-rmse:116076.53906\teval-rmse:211861.23438\n",
      "[81]\ttrain-rmse:115072.27344\teval-rmse:210111.84375\n",
      "[82]\ttrain-rmse:114169.32031\teval-rmse:208198.85938\n",
      "[83]\ttrain-rmse:114008.60156\teval-rmse:208608.43750\n",
      "[84]\ttrain-rmse:112822.15625\teval-rmse:208334.26562\n",
      "[85]\ttrain-rmse:112221.76562\teval-rmse:208673.51562\n",
      "[86]\ttrain-rmse:111500.66406\teval-rmse:206905.40625\n",
      "[87]\ttrain-rmse:110713.40625\teval-rmse:207033.40625\n",
      "[88]\ttrain-rmse:109521.71094\teval-rmse:206059.76562\n",
      "[89]\ttrain-rmse:108813.53125\teval-rmse:205233.96875\n",
      "[90]\ttrain-rmse:108291.40625\teval-rmse:203100.92188\n",
      "[91]\ttrain-rmse:106790.55469\teval-rmse:202940.09375\n",
      "[92]\ttrain-rmse:105809.30469\teval-rmse:202464.15625\n",
      "[93]\ttrain-rmse:104738.91406\teval-rmse:202583.64062\n",
      "[94]\ttrain-rmse:104071.34375\teval-rmse:202910.48438\n",
      "[95]\ttrain-rmse:102845.88281\teval-rmse:202709.92188\n",
      "[96]\ttrain-rmse:101486.80469\teval-rmse:203688.39062\n",
      "[97]\ttrain-rmse:100448.82812\teval-rmse:201470.18750\n",
      "[98]\ttrain-rmse:99453.78906\teval-rmse:201679.56250\n",
      "[99]\ttrain-rmse:98805.51562\teval-rmse:202217.53125\n",
      "[100]\ttrain-rmse:98457.89844\teval-rmse:201985.17188\n",
      "[101]\ttrain-rmse:98142.03125\teval-rmse:201155.60938\n",
      "[102]\ttrain-rmse:97221.77344\teval-rmse:200008.95312\n",
      "[103]\ttrain-rmse:96978.42188\teval-rmse:199646.75000\n",
      "[104]\ttrain-rmse:96080.12500\teval-rmse:199074.07812\n",
      "[105]\ttrain-rmse:94648.64844\teval-rmse:197747.21875\n",
      "[106]\ttrain-rmse:94271.95312\teval-rmse:197967.10938\n",
      "[107]\ttrain-rmse:93313.58594\teval-rmse:196205.31250\n",
      "[108]\ttrain-rmse:92541.07812\teval-rmse:195913.68750\n",
      "[109]\ttrain-rmse:92371.85156\teval-rmse:196423.10938\n",
      "[110]\ttrain-rmse:92364.35156\teval-rmse:196531.07812\n",
      "[111]\ttrain-rmse:92270.32812\teval-rmse:196522.07812\n",
      "[112]\ttrain-rmse:92207.42969\teval-rmse:196290.67188\n",
      "[113]\ttrain-rmse:91474.46094\teval-rmse:196083.03125\n",
      "[114]\ttrain-rmse:90621.92969\teval-rmse:194196.95312\n",
      "[115]\ttrain-rmse:90225.60938\teval-rmse:193835.85938\n",
      "[116]\ttrain-rmse:89170.32031\teval-rmse:193773.84375\n",
      "[117]\ttrain-rmse:88939.20312\teval-rmse:193517.78125\n",
      "[118]\ttrain-rmse:88081.79688\teval-rmse:193445.15625\n",
      "[119]\ttrain-rmse:87418.35938\teval-rmse:192515.89062\n",
      "[120]\ttrain-rmse:86610.93750\teval-rmse:191025.81250\n",
      "[121]\ttrain-rmse:86173.02344\teval-rmse:190891.54688\n",
      "[122]\ttrain-rmse:86060.56250\teval-rmse:190977.34375\n",
      "[123]\ttrain-rmse:85300.28125\teval-rmse:191089.79688\n",
      "[124]\ttrain-rmse:84945.30469\teval-rmse:189977.62500\n",
      "[125]\ttrain-rmse:84257.62500\teval-rmse:190103.82812\n",
      "[126]\ttrain-rmse:83676.86719\teval-rmse:190200.17188\n",
      "[127]\ttrain-rmse:83113.88281\teval-rmse:190312.67188\n",
      "[128]\ttrain-rmse:82754.74219\teval-rmse:189618.60938\n",
      "[129]\ttrain-rmse:82263.10938\teval-rmse:188714.70312\n",
      "[130]\ttrain-rmse:81710.57031\teval-rmse:187637.10938\n",
      "[131]\ttrain-rmse:81566.65625\teval-rmse:187457.57812\n",
      "[132]\ttrain-rmse:80729.21094\teval-rmse:186085.03125\n",
      "[133]\ttrain-rmse:80216.75000\teval-rmse:186133.04688\n",
      "[134]\ttrain-rmse:79788.25000\teval-rmse:186119.73438\n",
      "[135]\ttrain-rmse:79521.90625\teval-rmse:185711.10938\n",
      "[136]\ttrain-rmse:79122.42969\teval-rmse:185454.96875\n",
      "[137]\ttrain-rmse:78852.28125\teval-rmse:185888.75000\n",
      "[138]\ttrain-rmse:78456.29688\teval-rmse:184631.04688\n",
      "[139]\ttrain-rmse:77837.62500\teval-rmse:184243.82812\n",
      "[140]\ttrain-rmse:77429.52344\teval-rmse:183012.54688\n",
      "[141]\ttrain-rmse:77368.44531\teval-rmse:182515.40625\n",
      "[142]\ttrain-rmse:77062.58594\teval-rmse:183095.84375\n",
      "[143]\ttrain-rmse:77002.32812\teval-rmse:182695.51562\n",
      "[144]\ttrain-rmse:76310.08594\teval-rmse:181183.85938\n",
      "[145]\ttrain-rmse:76308.83594\teval-rmse:181787.87500\n",
      "[146]\ttrain-rmse:75935.45312\teval-rmse:180996.17188\n",
      "[147]\ttrain-rmse:75418.41406\teval-rmse:180985.62500\n",
      "[148]\ttrain-rmse:74880.53125\teval-rmse:180791.51562\n",
      "[149]\ttrain-rmse:74260.03906\teval-rmse:180798.85938\n",
      "[150]\ttrain-rmse:73977.65625\teval-rmse:180189.53125\n",
      "[151]\ttrain-rmse:73488.32812\teval-rmse:180155.10938\n",
      "[152]\ttrain-rmse:73178.89062\teval-rmse:180633.65625\n",
      "[153]\ttrain-rmse:72861.02344\teval-rmse:181042.81250\n",
      "[154]\ttrain-rmse:72510.74219\teval-rmse:181109.10938\n",
      "[155]\ttrain-rmse:72279.67188\teval-rmse:180597.28125\n",
      "[156]\ttrain-rmse:72038.82812\teval-rmse:180643.12500\n",
      "[157]\ttrain-rmse:71947.25000\teval-rmse:180760.53125\n",
      "[158]\ttrain-rmse:71188.26562\teval-rmse:179338.76562\n",
      "[159]\ttrain-rmse:71022.53125\teval-rmse:179405.57812\n",
      "[160]\ttrain-rmse:70506.46094\teval-rmse:178128.31250\n",
      "[161]\ttrain-rmse:70288.62500\teval-rmse:177624.25000\n",
      "[162]\ttrain-rmse:69920.07031\teval-rmse:178514.87500\n",
      "[163]\ttrain-rmse:69655.96875\teval-rmse:179074.79688\n",
      "[164]\ttrain-rmse:69570.85156\teval-rmse:179049.53125\n",
      "[165]\ttrain-rmse:69315.61719\teval-rmse:178346.06250\n",
      "[166]\ttrain-rmse:69192.98438\teval-rmse:177900.98438\n",
      "[167]\ttrain-rmse:68446.61719\teval-rmse:177304.73438\n",
      "[168]\ttrain-rmse:68378.06250\teval-rmse:176930.31250\n",
      "[169]\ttrain-rmse:67742.11719\teval-rmse:176767.10938\n",
      "[170]\ttrain-rmse:67667.43750\teval-rmse:176832.76562\n",
      "[171]\ttrain-rmse:67354.82812\teval-rmse:177438.90625\n",
      "[172]\ttrain-rmse:67347.36719\teval-rmse:177605.56250\n",
      "[173]\ttrain-rmse:67229.78125\teval-rmse:177564.01562\n",
      "[174]\ttrain-rmse:66979.11719\teval-rmse:176705.00000\n",
      "[175]\ttrain-rmse:66424.62500\teval-rmse:175272.45312\n",
      "[176]\ttrain-rmse:66201.67969\teval-rmse:175051.34375\n",
      "[177]\ttrain-rmse:66035.21875\teval-rmse:174969.84375\n",
      "[178]\ttrain-rmse:65768.69531\teval-rmse:175615.10938\n",
      "[179]\ttrain-rmse:65783.54688\teval-rmse:175980.70312\n",
      "[180]\ttrain-rmse:65478.15234\teval-rmse:176026.10938\n",
      "[181]\ttrain-rmse:65102.37500\teval-rmse:176244.07812\n",
      "[182]\ttrain-rmse:65021.07422\teval-rmse:176188.18750\n",
      "[183]\ttrain-rmse:64693.66797\teval-rmse:176057.32812\n",
      "[184]\ttrain-rmse:64447.59766\teval-rmse:175868.75000\n",
      "[185]\ttrain-rmse:64464.06641\teval-rmse:175413.23438\n",
      "[186]\ttrain-rmse:63716.53906\teval-rmse:174181.68750\n",
      "[187]\ttrain-rmse:63011.66016\teval-rmse:172589.34375\n",
      "[188]\ttrain-rmse:62810.08984\teval-rmse:172698.71875\n",
      "[189]\ttrain-rmse:62368.41797\teval-rmse:172907.12500\n",
      "[190]\ttrain-rmse:61992.72266\teval-rmse:172177.09375\n",
      "[191]\ttrain-rmse:61927.55078\teval-rmse:171899.06250\n",
      "[192]\ttrain-rmse:62000.92578\teval-rmse:171889.14062\n",
      "[193]\ttrain-rmse:61826.64453\teval-rmse:171484.50000\n",
      "[194]\ttrain-rmse:61443.44141\teval-rmse:170829.01562\n",
      "[195]\ttrain-rmse:61201.25000\teval-rmse:171177.95312\n",
      "[196]\ttrain-rmse:61125.97656\teval-rmse:171040.10938\n",
      "[197]\ttrain-rmse:60620.12500\teval-rmse:170279.53125\n",
      "[198]\ttrain-rmse:60388.97656\teval-rmse:170765.89062\n",
      "[199]\ttrain-rmse:60269.08984\teval-rmse:170411.51562\n",
      "[200]\ttrain-rmse:60210.20703\teval-rmse:170351.42188\n",
      "[201]\ttrain-rmse:60024.47656\teval-rmse:170386.48438\n",
      "[202]\ttrain-rmse:60005.78906\teval-rmse:170325.00000\n",
      "[203]\ttrain-rmse:59988.78516\teval-rmse:169994.03125\n",
      "[204]\ttrain-rmse:60013.99609\teval-rmse:169887.79688\n",
      "[205]\ttrain-rmse:59881.28516\teval-rmse:169671.79688\n",
      "[206]\ttrain-rmse:59629.54297\teval-rmse:169639.39062\n",
      "[207]\ttrain-rmse:59266.34766\teval-rmse:168751.01562\n",
      "[208]\ttrain-rmse:58841.43359\teval-rmse:168164.23438\n",
      "[209]\ttrain-rmse:58590.48438\teval-rmse:168137.98438\n",
      "[210]\ttrain-rmse:58332.68359\teval-rmse:168190.70312\n",
      "[211]\ttrain-rmse:57912.89844\teval-rmse:168248.25000\n",
      "[212]\ttrain-rmse:57951.72266\teval-rmse:168367.78125\n",
      "[213]\ttrain-rmse:57565.74609\teval-rmse:168528.59375\n",
      "[214]\ttrain-rmse:57317.72266\teval-rmse:169061.48438\n",
      "[215]\ttrain-rmse:57123.22266\teval-rmse:169143.28125\n",
      "[216]\ttrain-rmse:56836.67578\teval-rmse:168335.53125\n",
      "[217]\ttrain-rmse:56583.00781\teval-rmse:167812.31250\n",
      "[218]\ttrain-rmse:56502.59766\teval-rmse:167754.25000\n",
      "[219]\ttrain-rmse:56457.25000\teval-rmse:167887.18750\n",
      "[220]\ttrain-rmse:56118.64062\teval-rmse:167232.39062\n",
      "[221]\ttrain-rmse:55959.25781\teval-rmse:167415.85938\n",
      "[222]\ttrain-rmse:55623.30078\teval-rmse:167459.21875\n",
      "[223]\ttrain-rmse:55239.41797\teval-rmse:167463.96875\n",
      "[224]\ttrain-rmse:55020.96484\teval-rmse:166756.10938\n",
      "[225]\ttrain-rmse:54618.98047\teval-rmse:166382.78125\n",
      "[226]\ttrain-rmse:54427.38672\teval-rmse:166368.03125\n",
      "[227]\ttrain-rmse:54181.85156\teval-rmse:166006.18750\n",
      "[228]\ttrain-rmse:54035.63672\teval-rmse:166318.48438\n",
      "[229]\ttrain-rmse:53832.67578\teval-rmse:166831.07812\n",
      "[230]\ttrain-rmse:53658.25391\teval-rmse:166716.12500\n",
      "[231]\ttrain-rmse:53619.47656\teval-rmse:166304.28125\n",
      "[232]\ttrain-rmse:53653.94922\teval-rmse:166285.89062\n",
      "[233]\ttrain-rmse:53410.85938\teval-rmse:165663.01562\n",
      "[234]\ttrain-rmse:53359.44531\teval-rmse:165571.07812\n",
      "[235]\ttrain-rmse:53273.54297\teval-rmse:165288.31250\n",
      "[236]\ttrain-rmse:53001.19922\teval-rmse:164726.90625\n",
      "[237]\ttrain-rmse:52727.71875\teval-rmse:165271.35938\n",
      "[238]\ttrain-rmse:52765.72656\teval-rmse:165188.84375\n",
      "[239]\ttrain-rmse:52725.11328\teval-rmse:165076.25000\n",
      "[240]\ttrain-rmse:52747.60156\teval-rmse:164799.89062\n",
      "[241]\ttrain-rmse:52462.55469\teval-rmse:164968.90625\n",
      "[242]\ttrain-rmse:52176.66797\teval-rmse:164697.18750\n",
      "[243]\ttrain-rmse:52030.94141\teval-rmse:164867.18750\n",
      "[244]\ttrain-rmse:51837.50391\teval-rmse:164857.15625\n",
      "[245]\ttrain-rmse:51602.91016\teval-rmse:164404.93750\n",
      "[246]\ttrain-rmse:51523.67969\teval-rmse:164421.37500\n",
      "[247]\ttrain-rmse:51509.66406\teval-rmse:164056.07812\n",
      "[248]\ttrain-rmse:51368.96875\teval-rmse:164275.04688\n",
      "[249]\ttrain-rmse:51191.59766\teval-rmse:163678.51562\n",
      "[250]\ttrain-rmse:50921.37891\teval-rmse:163544.75000\n",
      "[251]\ttrain-rmse:50730.26953\teval-rmse:163679.31250\n",
      "[252]\ttrain-rmse:50481.60156\teval-rmse:164242.43750\n",
      "[253]\ttrain-rmse:50362.46875\teval-rmse:164504.82812\n",
      "[254]\ttrain-rmse:50150.32422\teval-rmse:164634.21875\n",
      "[255]\ttrain-rmse:49965.56250\teval-rmse:164553.48438\n",
      "[256]\ttrain-rmse:49950.10938\teval-rmse:164339.40625\n",
      "[257]\ttrain-rmse:49965.42578\teval-rmse:164240.54688\n",
      "[258]\ttrain-rmse:49934.46875\teval-rmse:164217.51562\n",
      "[259]\ttrain-rmse:49800.56641\teval-rmse:164610.92188\n",
      "[260]\ttrain-rmse:49734.22266\teval-rmse:164456.53125\n",
      "[261]\ttrain-rmse:49460.68750\teval-rmse:164478.85938\n",
      "[262]\ttrain-rmse:49311.53516\teval-rmse:164632.57812\n",
      "[263]\ttrain-rmse:49179.17969\teval-rmse:164605.43750\n",
      "[264]\ttrain-rmse:48975.92188\teval-rmse:164482.84375\n",
      "[265]\ttrain-rmse:48766.86328\teval-rmse:164348.81250\n",
      "[266]\ttrain-rmse:48620.58984\teval-rmse:164006.92188\n",
      "[267]\ttrain-rmse:48408.55859\teval-rmse:163469.56250\n",
      "[268]\ttrain-rmse:48214.89062\teval-rmse:163502.70312\n",
      "[269]\ttrain-rmse:47947.38281\teval-rmse:163635.23438\n",
      "[270]\ttrain-rmse:47905.42578\teval-rmse:163589.84375\n",
      "[271]\ttrain-rmse:47728.33594\teval-rmse:163426.90625\n",
      "[272]\ttrain-rmse:47548.93750\teval-rmse:163633.79688\n",
      "[273]\ttrain-rmse:47429.48438\teval-rmse:164074.84375\n",
      "[274]\ttrain-rmse:47071.26172\teval-rmse:163834.92188\n",
      "[275]\ttrain-rmse:47034.82812\teval-rmse:163855.31250\n",
      "[276]\ttrain-rmse:46872.11719\teval-rmse:163784.01562\n",
      "[277]\ttrain-rmse:46705.56641\teval-rmse:163288.31250\n",
      "[278]\ttrain-rmse:46596.39062\teval-rmse:163446.23438\n",
      "[279]\ttrain-rmse:46493.92578\teval-rmse:163528.10938\n",
      "[280]\ttrain-rmse:46431.78125\teval-rmse:163489.15625\n",
      "[281]\ttrain-rmse:46379.35938\teval-rmse:163742.45312\n",
      "[282]\ttrain-rmse:46332.72656\teval-rmse:163783.57812\n",
      "[283]\ttrain-rmse:46091.07812\teval-rmse:163362.10938\n",
      "[284]\ttrain-rmse:45915.91016\teval-rmse:162765.34375\n",
      "[285]\ttrain-rmse:45846.18359\teval-rmse:162792.31250\n",
      "[286]\ttrain-rmse:45825.64062\teval-rmse:162659.51562\n",
      "[287]\ttrain-rmse:45623.84766\teval-rmse:162798.06250\n",
      "[288]\ttrain-rmse:45536.33203\teval-rmse:162960.01562\n",
      "[289]\ttrain-rmse:45344.03516\teval-rmse:162667.09375\n",
      "[290]\ttrain-rmse:45366.49609\teval-rmse:162498.34375\n",
      "[291]\ttrain-rmse:45163.76562\teval-rmse:162085.21875\n",
      "[292]\ttrain-rmse:45150.76172\teval-rmse:162180.46875\n",
      "[293]\ttrain-rmse:45142.94141\teval-rmse:162101.28125\n",
      "[294]\ttrain-rmse:44977.42969\teval-rmse:162291.67188\n",
      "[295]\ttrain-rmse:44882.59766\teval-rmse:162240.71875\n",
      "[296]\ttrain-rmse:44797.03125\teval-rmse:162240.29688\n",
      "[297]\ttrain-rmse:44615.53125\teval-rmse:162386.68750\n",
      "[298]\ttrain-rmse:44607.69531\teval-rmse:162264.29688\n",
      "[299]\ttrain-rmse:44357.16797\teval-rmse:162458.04688\n",
      "[300]\ttrain-rmse:44235.92578\teval-rmse:162084.09375\n",
      "[301]\ttrain-rmse:44060.05469\teval-rmse:161723.25000\n",
      "[302]\ttrain-rmse:44007.08984\teval-rmse:161670.82812\n",
      "[303]\ttrain-rmse:43924.35156\teval-rmse:161898.51562\n",
      "[304]\ttrain-rmse:43677.88281\teval-rmse:161417.82812\n",
      "[305]\ttrain-rmse:43515.55859\teval-rmse:161567.40625\n",
      "[306]\ttrain-rmse:43397.31641\teval-rmse:161674.82812\n",
      "[307]\ttrain-rmse:43335.48047\teval-rmse:161581.85938\n",
      "[308]\ttrain-rmse:43198.26562\teval-rmse:161558.39062\n",
      "[309]\ttrain-rmse:43049.92969\teval-rmse:161820.07812\n",
      "[310]\ttrain-rmse:42856.26172\teval-rmse:161506.64062\n",
      "[311]\ttrain-rmse:42844.04297\teval-rmse:161538.68750\n",
      "[312]\ttrain-rmse:42735.00000\teval-rmse:161298.50000\n",
      "[313]\ttrain-rmse:42316.26562\teval-rmse:160720.10938\n",
      "[314]\ttrain-rmse:42206.73828\teval-rmse:161000.34375\n",
      "[315]\ttrain-rmse:42193.03906\teval-rmse:161012.39062\n",
      "[316]\ttrain-rmse:42095.98438\teval-rmse:161120.90625\n",
      "[317]\ttrain-rmse:41922.67188\teval-rmse:161310.03125\n",
      "[318]\ttrain-rmse:41774.73828\teval-rmse:161428.10938\n",
      "[319]\ttrain-rmse:41684.92969\teval-rmse:161589.89062\n",
      "[320]\ttrain-rmse:41536.72266\teval-rmse:161376.96875\n",
      "[321]\ttrain-rmse:41480.84766\teval-rmse:161686.89062\n",
      "[322]\ttrain-rmse:41333.44141\teval-rmse:161879.67188\n",
      "[323]\ttrain-rmse:41195.85938\teval-rmse:161686.21875\n",
      "[324]\ttrain-rmse:41090.32812\teval-rmse:161931.25000\n",
      "[325]\ttrain-rmse:41028.16406\teval-rmse:161644.28125\n",
      "[326]\ttrain-rmse:40928.51953\teval-rmse:161775.31250\n",
      "[327]\ttrain-rmse:40905.87109\teval-rmse:161445.21875\n",
      "[328]\ttrain-rmse:40764.07812\teval-rmse:160994.79688\n",
      "[329]\ttrain-rmse:40756.17969\teval-rmse:161139.76562\n",
      "[330]\ttrain-rmse:40633.10156\teval-rmse:161193.92188\n",
      "[331]\ttrain-rmse:40563.97656\teval-rmse:161220.78125\n",
      "[332]\ttrain-rmse:40539.55859\teval-rmse:161179.14062\n",
      "[333]\ttrain-rmse:40341.43750\teval-rmse:160954.40625\n",
      "[334]\ttrain-rmse:40043.39844\teval-rmse:160521.68750\n",
      "[335]\ttrain-rmse:39947.79297\teval-rmse:160368.68750\n",
      "[336]\ttrain-rmse:39814.10938\teval-rmse:160129.43750\n",
      "[337]\ttrain-rmse:39747.55078\teval-rmse:160041.84375\n",
      "[338]\ttrain-rmse:39638.83984\teval-rmse:160186.29688\n",
      "[339]\ttrain-rmse:39645.86328\teval-rmse:160216.07812\n",
      "[340]\ttrain-rmse:39520.65625\teval-rmse:159983.18750\n",
      "[341]\ttrain-rmse:39458.43359\teval-rmse:160256.67188\n",
      "[342]\ttrain-rmse:39492.68359\teval-rmse:160151.07812\n",
      "[343]\ttrain-rmse:39456.31250\teval-rmse:160110.42188\n",
      "[344]\ttrain-rmse:39326.29297\teval-rmse:160044.17188\n",
      "[345]\ttrain-rmse:39327.25781\teval-rmse:160056.78125\n",
      "[346]\ttrain-rmse:39260.63672\teval-rmse:159936.26562\n",
      "[347]\ttrain-rmse:39161.99609\teval-rmse:159899.18750\n",
      "[348]\ttrain-rmse:39054.93359\teval-rmse:160045.50000\n",
      "[349]\ttrain-rmse:39051.86719\teval-rmse:159821.82812\n",
      "[350]\ttrain-rmse:38859.76562\teval-rmse:159867.51562\n",
      "[351]\ttrain-rmse:38774.05469\teval-rmse:159618.39062\n",
      "[352]\ttrain-rmse:38704.48047\teval-rmse:160010.75000\n",
      "[353]\ttrain-rmse:38701.69141\teval-rmse:159941.53125\n",
      "[354]\ttrain-rmse:38596.25781\teval-rmse:159795.46875\n",
      "[355]\ttrain-rmse:38491.97656\teval-rmse:160152.70312\n",
      "[356]\ttrain-rmse:38378.84766\teval-rmse:159679.28125\n",
      "[357]\ttrain-rmse:38260.19531\teval-rmse:159771.85938\n",
      "[358]\ttrain-rmse:38212.97266\teval-rmse:159654.28125\n",
      "[359]\ttrain-rmse:38010.60547\teval-rmse:159246.17188\n",
      "[360]\ttrain-rmse:37888.69922\teval-rmse:159176.25000\n",
      "[361]\ttrain-rmse:37862.36719\teval-rmse:159345.53125\n",
      "[362]\ttrain-rmse:37838.00391\teval-rmse:159457.26562\n",
      "[363]\ttrain-rmse:37709.91016\teval-rmse:159561.84375\n",
      "[364]\ttrain-rmse:37616.51562\teval-rmse:159690.21875\n",
      "[365]\ttrain-rmse:37510.84375\teval-rmse:159684.46875\n",
      "[366]\ttrain-rmse:37497.50781\teval-rmse:159545.37500\n",
      "[367]\ttrain-rmse:37450.48828\teval-rmse:159485.12500\n",
      "[368]\ttrain-rmse:37341.08984\teval-rmse:159570.26562\n",
      "[369]\ttrain-rmse:37241.91016\teval-rmse:159317.68750\n",
      "[370]\ttrain-rmse:37162.72266\teval-rmse:159383.48438\n",
      "[371]\ttrain-rmse:37076.85156\teval-rmse:159196.12500\n",
      "[372]\ttrain-rmse:36973.68750\teval-rmse:159365.64062\n"
     ]
    }
   ],
   "source": [
    "num_boost_round = len(xgbCV)\n",
    "parameters['eval_metric'] = 'rmse'\n",
    "\n",
    "xgbFinal = xgb.train(\n",
    "    params = parameters,\n",
    "    dtrain = trainDMat, \n",
    "    num_boost_round = num_boost_round,\n",
    "    evals = [(trainDMat, 'train'), \n",
    "             (testDMat, 'eval')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffdd427",
   "metadata": {},
   "source": [
    "绘制特征重要性图并打印数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4399361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5MAAAJXCAYAAAAO4QCfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABAO0lEQVR4nO3dfZRddX0v/ve3oDSS5qEFScVK4OLCUiBoEBKviRNqg2mgjbbGUnqLEI2V1j6gl59IpNAiwr3KRbhYCz5QolQebrjVUipj4VyIDdIQDYG0WaG5CJULBZNYk0YTJt/fH3NM8zCE2ZqZc2byeq01a/be57u/57PPZ80k79n77FNqrQEAAIAmfqLTBQAAADDyCJMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAjAqlFIuKaVsKqU8vdPXOzpdV1OllE+MxLoB2P8UnzMJwGhQSrkkyU/WWj+4D+aanKSn1nrjjzvXSLC/HS8A+4YzkwCwp8lJ3tnhGobT5OxfxwvAPiBMAjDqlVI+VEp5opTyrVLKGe1tP1FKub6U8lQp5bFSyuz29vuTLEnyhvalsje0t1/SPvv5wzkfb5/R++HyqaWUe0spf7HTmHNKKetKKf+vlPLuQdZ6Yynlnbs9z83tOT5aSvnXUspvt8fdWkr5v6WUNaWUaTsd18dLKd8upawspbx+p7lqKeV1pZTlpZQ/fZHjfaHX55JSyv8opfxtKeU7pZRP7DT+Y+3nXfvD8S/0+gMw8h3Y6QIAYB/6vZ2C2IdrrTeUUuYk+cUkP5/klUlapZRXJXldkp9O8qokJyW5JsndtdYZpZSeJJfUWnsaPPfHkvx+kpVJUkr5hSTnt+d+SZIVpZQv1Vqf+RGO62+SvDTJoUn+JMkP6/q5JK9JMjvJje3lc5OcmOQ/JXlDkttKKcfUWn/Q3ue/J1mQZG2S7OV4X58BXp/2Y+cmeXOSp5KsK6VcmuTXkkxpP+/rktye5BUv9PrXWrf9CK8DAF1EmARgNPmfA7xn8s3pD0b/3F5/WZJX1Fq/Xkq5Kv3h7JfSH9SaKLut/7da69Kd1k9NclSS1e31MUmOSfKjhMll6T+OZUn68h9XFv1lOyR+uZTyl6WUCUnmJLmh1vr9JPeUUr6b5Pgky9v7XFRrXfliT/gir8+Xa63/kCSllKeTjEvyliSfbj/v3yd5RXvsgK9/km81ewkA6DYucwVgtCtJPlJrnVRrnZT+M23fLqWcleS6JN9I8nuNJizlwCSH7bb5gQGe96adnveVA4wZrL7dvu/8HDsvb28v73x3vbrzeq11UDW8yOvzzzstD3gnv/YlvmPzAq//YGoAoLsJkwCMdl9NMr+UMq6U8or0B6EJSaYn+UqSv0ryq7vt81ySV5ZSDiilTCylHJDk39J/WWmSLExy0Is87z1J5pRSJpVSfir9l78euy8OaCe/WUr5yVLKvCT/t9b6b0nuSrKglHJQKeVN6T/WR15knoGOd2+vz0AB8itJzm0/73FJrkjy/bzw6w/ACOcyVwBGtVrr35RSpqY/UPUleV+t9blSyo1Jbk3yX5L8ZZJDSynja63frbU+Ukr5avrPoPUlOTrJF5PcVUq5O/2Xm+71Ms32HH/aHntgkqtrrd/cx4e3NsmjSZ5vH0eSfDb9709cl/6Q+Pad3i+5t1p3P94bM8Drs5dpPrPT8/5bkt+stT6fZMDXv/mhAtBtfM4kAIxA7TDc8tmQAHSKy1wBAABozJlJAAAAGnNmEgAAgMaESQAAABpzN9e9mDBhQj366KM7XQZJNm/enIMPPrjTZez39KF76EV30IfuoRfdQR+6gz50j9HQi4ceeui5WuuhAz0mTO7FYYcdluXLl3e6DJK0Wq309PR0uoz9nj50D73oDvrQPfSiO+hDd9CH7jEaelFKecGPwnKZKwAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQWKm1drqGrvWqo46uPzH/E50ugyTvP/75fHzVgZ0uY7+nD91DL7qDPnQPvegO+tAd9GHvHr9i7rA9V6vVSk9Pz7A931AopTxUaz1poMecmQQAAHgB69evT29vb5577rlOl9J1RmyYLKW0dlu/eqfld5ZS3jnMJQEAACPEn/3Zn6Wnpyc9PT058cQTc9BBB+2y/p73vCcbNmzI6aefngcffDCzZs3Ks88++4LzLViwINOnT89ll102jEfRWSM2TO6u1vqHna4BAAAYGd773vem1Wql1WplxowZ+drXvrbL+rvf/e48/PDDueqqq3LRRRfltNNOy4oVKwaca8mSJenr68uyZcuybt26rF27dpiPpjNGTZjc/Uxle9svlFLuLaX8VCnlZaWU20sp95VSrutAiQAAQJf59re/nWeeeSYnnXTSHutvetObMm3atNx333158MEHM3369AHnaLVamT9/fpJk9uzZWbp06bDV30mj+Z25P5vkC0neUmv9XinlD5M8Umu9pJSypJRyQq314d13KqUsTLIwSQ455NBcfPzzw1o0AztsTP+byeksfegeetEd9KF76EV30IfuoA9712q1dlm/4YYbMn369B3bd1+vteYTn/jEjjOPBx100B5zPvbYY3nqqafSarXy5JNPZu3atTnyyCOzadOmPZ5vNBnNYfL3knwjyRFJnk5yTJI3lFJ6kkxIcniSPcJkrfX6JNcn/XdzdSes7uCuZN1BH7qHXnQHfegeetEd9KE76MPePX5Wz47l7du358ILL8znP//5lFL2WP+hWbNm5cMf/nA2btyYd7zjHXvMeccdd+S4447LtGnTsn79+mzbti09PT2j4m6uezNqLnMdwJ8meW/7e5KsSXJ1rbUnyaIkT3SoLgAAoAvcf//9OeWUU3YEx93Xr7zyytx0001Jko0bN2bChAkDzjN16tQdl7auXLkykydPHvLau8FoDpPfr7U+meSfSim/kuSGJHNKKfcl+Z0kT3a0OgAAoKO+8pWvZObMmS+4vnDhwixevDgzZ85MX19fZs+endWrV2fRokW7zDNv3rwsXrw4559/fm699dbMnTt8n2XZSSP2/Hf7DOOA67XWG3da/v2dhs0f6roAAICR4fLLL9/r+sSJE9Pb27vLtmOPPXaPj/8YN25cWq1Went7c8EFF2T8+PFDU3CXGbFhcjiMeckBWXPF/vFXhW7XarV2ub6dztCH7qEX3UEfuodedAd96A760BkTJ07ccUfX/cVovswVAACAISJMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0NiBnS6gm23Z1pfJH7yz02WQ5P3HP5936kXH6UP30IvuoA/dQy+6w1D24fEr5g7JvMCPzplJAAAAGhMmAQAYVdavX5/e3t4899xznS4FRrUhCZOllNZu61fvtPzOUso7h+J5d3vOE0spJw718wAAMLzOO++8fPnLX853v/vdzJkzJ7Nnz85b3/rWbN26NRs2bMjpp5+eBx98MLNmzcqzzz77gvMsWLAg06dPz2WXXTaM1cPoMSxnJmutfzgcz7ObE9tfAACMEvfff3+efvrpnHHGGfnCF76Q888/P3fffXcmTZqUv/3bv83DDz+cq666KhdddFFOO+20rFixYsB5lixZkr6+vixbtizr1q3L2rVrh/lIYOQrtdZ9P2kprVprz0DrO52V/Ick/zPJryTpS3JTkpcnWVVr/d29zd3e94Ra62mllLFJbk9ycJLHaq3nlFI+muSt7V2+XWv9xVLKywbzHKWUhUkWJskhhxw69eKrb2j+ArDPHTYmeWZLp6tAH7qHXnQHfegeetEdhrIPxx8+Ps8//3zOPffcnHLKKZkyZUre+MY37nj8j//4j/OOd7wjxx57bJJk5cqV+exnP5vLL788Bx988B7zXXPNNTn55JMzbdq03HPPPfnBD36QOXPmDE3xw2zTpk0ZO3Zsp8sgo6MXs2bNeqjWetJAj3Xqbq4/m+QLSd5Sa/1eKeUPkzxSa72klLKklHJCrfXhF9h3WpJraq3/dae5rk3y1SR/W0o5rNZ6YSllTZLUWm9sj1s4mOeotV6f5PokedVRR9ePr3LD227w/uOfj150nj50D73oDvrQPfSiOwxlHx4/qyef+cxnctJJJ+Xaa6/Ntddem1WrVuV973tfli1blgMPPDDnnXdekqTWmttuuy2TJ0/OqaeemjFjxuwx3+LFizNnzpxMmTIlW7duzYoVK9LT0zMktQ+3Vqs1ao5lpBvtvejUDXh+L8m/JDmivX5Mkre2zzoeleTwvez7SK11yU7r25K8K/3h9KeT7PnbovlzAADQZb7xjW9k4cKFmTRpUn7rt34r9957b9avX5/3ve99+exnP7tjXCkl1113XU444YR86UtfGnCusWPHZsuW/tOomzZtyvbt24flGGA06VSY/NMk721/T5I1Sa5uXwq7KMkTe9l3027rC9J/meuZSTbvtH1LkpclSSmlNHwOAAC6zNFHH51169YlSZYvX54jjjgib3/72/PRj340RxzRf47iyiuvzE033ZQk2bhxYyZMmDDgXFOnTs3SpUuT9F8SO3ny5CGvH0abToXJ79dan0zyT6WUX0lyQ5I5pZT7kvxOkicbzNWb5MIk97TXD99p+9tKKV9LMuPHfA4AADpswYIFuffeezNz5sx88pOfzCGHHJIVK1bkIx/5SHp6enLLLbdk4cKFWbx4cWbOnJm+vr7Mnj07q1evzqJFi3aZa968eVm8eHHOP//83HrrrZk7d26HjgpGriG5Ac9occwxx9Q1a9Z0ugwy+q83Hyn0oXvoRXfQh+6hF91hpPVhw4YN6e3tzcyZMzNp0qROl7PPjLQ+jGajoRellK67Ac+L2v2zKpN8t9b6q52oBQCA0WfixImZP39+p8uAEatrw+TOHy0CAABAd+nUeyYBAAAYwYRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxg7sdAHdbMu2vkz+4J2dLoMk7z/++bxTLzpOH7qHXnSHH7cPj18xdx9WAwDDy5lJAAAAGhuxYbKUcmIp5cTdtl3dmWoA4MfzzDPP5LWvfe2O5RkzZuwx5rzzzsuXv/zlvc6zYMGCTJ8+PZdddtmQ1AkAPzRiw2SSE9tfO9Ra/7AThQDAj+sDH/hAtmzZkg0bNuTss8/O5s2bd3n8/vvvz9NPP50zzjjjBedYsmRJ+vr6smzZsqxbty5r164d6rIB2I+VWmuna2islPLRJG9tr3671vqL7e2tWmtPe/mhJP+aZGuSSUk+l+SOJDcmGZ/ky7XWjw4w98IkC5PkkEMOnXrx1TcM6bEwOIeNSZ7Z0ukq0IfuoRfd4cftw/GHj0+SrFixIq1WK0888UQ+8pGPpNaaRYsW5eqrr06SPP/88zn33HNzyimnZMqUKXnjG9844HzXXHNNTj755EybNi333HNPfvCDH2TOnDk/eoEjyKZNmzJ27NhOl7Hf04fuoA/dYzT0YtasWQ/VWk8a6LEReQOeWuuFpZQ17eUbX2DYy5K8PcmqJG9OsijJa5LcUmu9sZTy9VLK9bXW7+w29/VJrk+SVx11dP34qhH5Eo067z/++ehF5+lD99CL7vDj9uHxs3qydevWXHrppbnjjjsyb968zJ3bf1Oej33sY+np6UmSfOYzn8lJJ52Ua6+9Ntdee21WrVqV973vfXvMt3jx4syZMydTpkzJ1q1bs2LFih1zjHatVmu/OdZupg/dQR+6x2jvxUi+zPXFPFNr3ZTkW0n6kpQkxyR5bymlleTgJK/oXHkAkFxxxRU577zzMmHChBcc841vfCMLFy7MpEmT8lu/9Vu59957Bxw3duzYbNnSf6p006ZN2b59+1CUDABJRnaY3JL+s48ppZRB7rMmyQfbl8JekWT90JQGAIPz1a9+Ndddd116enryzW9+M+9617v2GHP00Udn3bp1SZLly5fniCOOGHCuqVOnZunSpUmSlStXZvLkyUNWNwCM5GukepPcWko5K8mFSe4bxD5XJPlMKeWyJP83yReHsD4AeFH33fcf/3z19PTk05/+9B5jFixYkHPPPTdf/OIXs23bttx+++1ZvXp1br755l3u2jpv3rzMmDEjTz31VO6666488MADw3IMAOyfRuQNeIbLMcccU9esWdPpMsjov958pNCH7qEX3aEb+7Bhw4b09vZm5syZmTRpUqfLGTbd2Iv9kT50B33oHqOhF6WU0XUDHgBgYBMnTsz8+fM7XQYA+4GR/J5JAAAAOkSYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoLEDO11AN9uyrS+TP3hnp8sgyfuPfz7v1IuO04fusS968fgVc/dRNQDA/siZSQCyfv369Pb25rnnnut0KQDACDEiwmQppTWYbYOc68RSyok/ZkkAo8aGDRty+umn58EHH8ysWbPy7LPP5plnnsmMGTN2GfPLv/zLOemkk/Ke97xnr/MtWLAg06dPz2WXXTbUpQMAHTQiwuQ+dmL7C4AkDz/8cK666qpcdNFFOe2003LPPffk7LPPzubNm3eMWbx4cc4666wsX7483/ve97J8+fIB51qyZEn6+vqybNmyrFu3LmvXrh2uwwAAhlnHwmQp5SdLKV8spSwtpfx1KWV8KeUvSyn/p5TyhVLKSxvM9bJSyu2llPtKKde9wPwvK6V8NMkHk3ywlPJ3Q3VsACPJm970pkybNi333XdfHnzwwcyZMye33HJLxo0bt2PMz/zMz+SRRx7Jxo0b8+STT+bnfu7nBpyr1Wpl/vz5SZLZs2dn6dKlw3IMAMDw6+QNeBYmWVlr/Y1SyjlJ/iDJI7XWM0splyQ5N8mnGsz1SK31klLKklLKCUl6dpv/uFrrhaWUNUlSa71xoIlKKQvb8+WQQw7Nxcc//6MfIfvMYWP6bzhCZ+lD99gXvWi1WjuWa635xCc+seOs4kEHHZSNGzfuGHPAAQfkwQcfzB/90R9l/PjxWbVqVf7xH/9xjzkfe+yxPPXUU2m1WnnyySezdu3aHHnkkT9Wnd1s06ZNu7yOdI5edAd96A760D1Gey86GSZfk+R/tZdvTPLJndYfSDKnwVzHJHlDKaUnyYQkhw8w/6DUWq9Pcn2SvOqoo+vHV7nhbTd4//HPRy86Tx+6x77oxeNn9eyyPmvWrHz4wx/Oxo0b8453vCMTJkxIT0//mHPPPTd33HFHxo0bl6uuuirr1q3LwoUL95jzjjvuyHHHHZdp06Zl/fr12bZt2445RqNWqzWqj28k0YvuoA/dQR+6x2jvRSffM/lPSV7fXv5Qe31ae31akkcbzLUmydW11p4ki5I8McD872ovb0nysiQppZQfsXaAUePKK6/MTTfdlCTZuHFjJkyYsMeYDRs2ZNWqVenr68vXv/71vNCvz6lTp+64tHXlypWZPHnyUJUNAHRYJ8PkDUle174r6+vSf/bwF0op9yV5dRqcTWzPNae97+8keXKA+Re3x/YmeVsp5WtJZgwwF8B+ZeHChVm8eHFmzpyZvr6+zJ49e48xF154YRYuXJjx48dn/fr1OfPMM7N69eosWrRol3Hz5s3L4sWLc/755+fWW2/N3Lk+yxIARquOXa9Wa92SZP5um898gbE9e9tWa908wFwZaFutdX2SNzcoFWBUmzhxYnp7e/fYvvN7PE4++eQ8+uiuF4wce+yxe3z8x7hx49JqtdLb25sLLrgg48ePH5KaAYDO8+anvRjzkgOy5gp/Ve8GrVZrj/d3Mfz0oXt0cy8mTpy4446uAMDotT9+ziQAAAA/JmESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxg7sdAHdbMu2vkz+4J2dLoMk7z/++bxTLzpOH7rHYHrx+BVzh6kaAGB/5MwkAAAAjQmTAPuB9evXp7e3N88991ynSwEARokRFSZLKZeUUno6XQfASLJhw4acfvrpefDBBzNr1qw8++yzeeaZZzJjxoxdxi1YsCDTp0/PZZddttf5BjsOABjdRlSYBKC5hx9+OFdddVUuuuiinHbaabnnnnty9tlnZ/PmzTvGLFmyJH19fVm2bFnWrVuXtWvXDjjXYMcBAKNfqbV2uoa9KqVMTHJbkgOSlCQfS/J7SQ5O8lit9ZxSyiVJXpJkRpJxSd6S5M4k/5pka5JJST6XZFqSS2qtj7f3adVaW7s938IkC5PkkEMOnXrx1TcM8REyGIeNSZ7Z0ukq0IfuMZheHH/4+F3WV65cmc9+9rO5/PLLU2vNokWLcvXVVydJrrnmmpx88smZNm1a7rnnnvzgBz/InDlz9phzsOP2F5s2bcrYsWM7XQbRi26hD91BH7rHaOjFrFmzHqq1njTQYyPhbq4Lk/x1rfXqUkpvkp9Ncm2Sryb521LKYe1xR9daZ5ZSLk5yapKXJXl7klVJ3pxk0WCerNZ6fZLrk+RVRx1dP75qJLxEo9/7j38+etF5+tA9BtOLx8/q2bFca81tt92WyZMn59RTT82YMWPysY99LD09/WMWL16cOXPmZMqUKdm6dWtWrFix47GdDXbc/qLVau3Xx99N9KI76EN30IfuMdp7MRIucz0yycr28vIk25K8K8kXkvx0kjHtx25qf38iyUuTPFNr3ZTkW0n60n9Wc2djArCfKKXkuuuuywknnJAvfelLezw+duzYbNnSf6pz06ZN2b59+4DzDHYcADD6jYQw+USSX2gvn5hkQZLbk5yZZPNO4zbnxW1Ncmgp5YAkv7QPawToWldeeWVuuqn/720bN27MhAkT9hgzderULF26NEn/5bCTJ08ecK7BjgMARr+RcL3a9UluK6X8evrfF/mVJBcm+Z3244c3mOuLSa5M8lj7C2DUW7hwYebPn59Pf/rTOe644zJ79uw9xsybNy8zZszIU089lbvuuisPPPBAVq9enZtvvnmXu7YONA4A2D91/Q14OumYY46pa9as6XQZZPRfbz5S6EP3GIpebNiwIb29vZk5c2YmTZr0Y4/bH/iZ6B560R30oTvoQ/cYDb0opYzoG/AAMAwmTpyY+fPn77NxAMDoNhLeMwkAAECXGVSYLKX8RCllXCnlwFLKrFLKTw11YQAAAHSvwZ6ZvC3JzCT/I/0fy3HHkFUEAABA1xtsmPyZWutfJ3l1rfWs+IxGAACA/dpgw+T3Sin/O8lDpZRfTvK9oSsJAACAbjfYu7m+PcmxtdYVpZQpSd4xhDUBAADQ5QZ1ZrLW+v0kW0sppyXZmqRvSKsCAACgqw32bq7XJrk0yUeTHJXk5qEsCgAAgO422PdMHl9r/bUkG2utdyYZP4Q1AQAA0OUGGyafLaVcnGRiKeXsJE8PYU0AAAB0ucGGyd9O8t0ky9J/VvKcIasIAACArjeou7nWWrck+cQQ1wIAAMAIMdgb8Nw11IUAAAAwcgz2MtdVpZRfHdJKAAAAGDEGdZlrktcneV8pZVWSzUlqrfXUoSsLAACAbjbY90zOGupCAAAAGDkGFSZLKb+9+7Za6037vhwAAABGgsG+Z7K0v16W5G1JZg5ZRQAAAHS9wV7m+hc7rX6qlPLJIaoHAACAEWCwl7nufCby0CS/MDTlAAAAMBIM9m6uO9+AZ2uS84agFgAAAEaIwV7meunO66WUNw5NOQAAAIwEg7oBTymld7dNHx2CWgAAABgh9npmspRyQpLXJjl8p48HOTjJ94e6MAAAALrXi52ZLAN8/06S+UNWEQAAAF1vr2cma60rk6wspRxTa71pmGoCAACgyw32BjwfKqUcmmRMe9PhtdZlQ1cWAAAA3WywnzP5mSRHJpmY5N+T1CTu6AoAALCfGtTdXJMcneQtSR5L8qYk24esIgAAALreYMPkvyf5xSQHJHl7+s9QAgAAsJ8abJj89SRrk/xRkp9Pct6QVQQAAEDXG+wNeDaXUn4yyX9K8pdJnhzSqgAAAOhqgzozWUq5NsmlST6a5KgkNw9lUQAAAHS3wV7menyt9deSbKy13plk/BDWBAAAQJcbbJh8tpRycZKJpZSzkzw9hDUBAADQ5fYaJkspP7zRzm8n+W6SZek/K3nOENcFAABAF3uxG/D8epJP1lq3lFJeXWt1F1cAAAAGfZlrkhw7ZFUAAAAworzYmclJpZTfTFJ2Wk6S1FpH/R1dt2zry+QP3tnpMkjy/uOfzzv1ouP0Ye8ev2Jup0sAABg2LxYmb0ny6gGW65BVBLAfWL9+fR566KG89rWvzSGHHNLpcgAAGttrmKy1XjpchQxWKeWSJK1aa6vDpQDs4bvf/W5+4zd+I319fTn44IPT09OTO+64I0mycePGnHLKKbniiity+umnZ+7cuTn//PNzzz335NBDDx1wvgULFmT16tWZO3duFi1aNJyHAgCwV03eMwnAi/jCF76Q888/P3fffXcmTZqUI488Mq1WK61WKzNmzMi73/3uPPzww7nqqqty0UUX5bTTTsuKFSsGnGvJkiXp6+vLsmXLsm7duqxdu3aYjwYA4IW92GWuHVdKGZPktiTjknwnyaNJfqmU8iftbW9JsinJ7UkOTvJYrfWc9r6tJP+Q5IRa62mllJcluSnJy5OsqrX+7jAfDjDKnXfef9z0+tlnn83LX/7yJMm3v/3tPPPMMznppJN2PH7fffflwQcfzMUXXzzgXK1WK/Pnz0+SzJ49O0uXLs2rX/3qAccCAAy3rg+T6b+L7PZa68xSyq8kOTXJz7bXL26v/0OSa5N8NcnfllIOq7U+k2Rakmtqrf+1PdfCJI/UWi8ppSwppZxQa3145ycrpSxsj8shhxyai49/flgOkr07bEz/zV/oLH3Yu1artWP50Ucfzbp16/L9738/rVYrN9xwQ6ZPn75jTK01n/jEJ3aceTzooIP2mO+xxx7LU089lVarlSeffDJr167NkUcemSTZtGnTLs9HZ+hD99CL7qAP3UEfusdo78VICJMrkjxSSrk7ydok303/2cUkeSLJS5NsS/KuJOck+ekkY9qPP1JrXbLTXMckeUMppSfJhCSHJ9klTNZar09yfZK86qij68dXjYSXaPR7//HPRy86Tx/27vGzepL031znAx/4QO64444cccQR2b59ey688MJ8/vOfTyllx/hZs2blwx/+cDZu3Jh3vOMde8x3xx135Ljjjsu0adOyfv36bNu2LT09/c/RarV2LNM5+tA99KI76EN30IfuMdp7MRLeMzklyddqrbOTTEwyI8nm3cYsSP9lrmfu9tim3catSXJ1rbUnyaL0h1GAfWbr1q15+9vfno9+9KM54ogjkiT3339/TjnllB1B8sorr8xNN/X/TWzjxo2ZMGHCgHNNnTo1S5cuTZKsXLkykydPHvL6AQAGaySEyceT/H4p5e+TTEqyfIAxvUkuTHJPe/3wF5jrhiRzSin3JfmdJE/u21KB/d1nPvOZrFixIh/5yEfS09OTW265JV/5ylcyc+bMHWMWLlyYxYsXZ+bMmenr68vs2bOzevXqPe7WOm/evCxevDjnn39+br311syd63MsAYDu0fXXq9VaNyY57QUeu3Gn1eMGeLxnt/XNSebvu+oAdvXe9743733ve3fZtvslrBMnTkxvb+8u24499thcdtllu2wbN25cWq1Went7c8EFF2T8+PFDUzQAwI+g68NkJ415yQFZc4UzAd2g1WrteD8anaMPw2/ixIk77ugKANBNRsJlrgAAAHQZYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGDux0Ad1sy7a+TP7gnZ0ugyTvP/75vFMv9pnHr5jb6RIAABjhnJkEAACgMWESAACAxkZVmCylXN3pGmCkeeaZZzJjxowkyYYNG/LLv/zLOemkk/Ke97xnl3HnnXde/v7v/36vcy1YsCDTp0/PZZddNmT1AgDQHUZVmKy1/mGna4CRZMOGDTn77LOzefPmJMnixYtz1llnZfny5fne976X5cuXJ0nuv//+PP3003nDG97wgnMtWbIkfX19WbZsWdatW5e1a9cOyzEAANAZpdba6Rr2qpTyk0luTPLKJBuTnJXkU0lekeRfkpxTa93aHtuqtfa0ly9J8pIkM5KMS/KWJN9Nclt7/TtJ3l5rfX6351uYZGGSHHLIoVMvvvqGoTw8BumwMckzWzpdxehx/OHjkySbN29OrTWLFi3K1Vdfnd7e3jz++OM588wz86EPfSiXXHJJxo0bl3PPPTennHJKjjnmmLz5zW8ecM5rrrkmJ598cqZNm5Z77rknP/jBDzJnzpzhPKz9yqZNmzJ27NhOl7Hf04fuoRfdQR+6gz50j9HQi1mzZj1Uaz1poMdGwt1cFyZZWWv9jVLKOUn+IMkjtdYz24Hx3PSHy4EcXWudWUq5OMmpSdYk2d7e9itJxqY/oO5Qa70+yfVJ8qqjjq4fXzUSXqLR7/3HPx+92HceP6tnl/WPfexj6enpyZFHHpkLL7wwK1asyLRp03LGGWfkpptuykknnZRrr702H/jAB7Jq1aq8733v22POxYsXZ86cOZkyZUq2bt2aFStWpKenZ49x7ButVsvr2wX0oXvoRXfQh+6gD91jtPdiJFzm+pokD7aXb0zys0m+3l5/IMnP72Xfm9rfn0jy0iQrkjxSSrk7yWlJ/n1fFwsj2aWXXppPfepTufjii/Oa17wmn/vc5/KNb3wjCxcuzKRJk/JLv/RLuffeewfcd+zYsdmypf/08aZNm7J9+/bhLB0AgGE2EsLkPyV5fXv5Q+31ae31aUke3cu+m3dbn5Lka7XW2Ukmpv8SWKBtw4YNWbVqVfr6+vL1r389pZQcffTRWbduXZJkzZo1OeKIIwbcd+rUqVm6dGmSZOXKlZk8efJwlQ0AQAeMhOsGb0jyF6WUVvrf53hukk+VUu5L8mSSyxvM9XiSK0spFyX5fpLl+7ZUGNkuvPDCnHPOOfnWt76V6dOn58wzz0ytNeeee26++MUv5jvf+U7uvvvurF69OjfffPMud22dN29eZsyYkaeeeip33XVXHnjggQ4eCQAAQ63rw2StdUuS+bttPvMFxvbstHzJTss37jTstME+95iXHJA1V8wd7HCGUKvV2uN9fuw7rVYrSXLyySfn0Uf3PNl/22237Rh3+OGH5/DDD9/j4z/GjRuXVquV3t7eXHDBBRk/fvyQ1w0AQOd0fZgERo6JEydm/vzd//YDAMBoNBLeMwkAAECXESYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABo7MBOF9DNtmzry+QP3tnpMkjy/uOfzzv38148fsXcTpcAAAA7ODMJAABAY6M+TJZSWp2uAfa1Z555JjNmzNhl23nnnZcvf/nLL7ptdwsWLMj06dNz2WWX7fM6AQAYvUZ9mITRZsOGDTn77LOzefPmHdvuv//+PP300znjjDP2um13S5YsSV9fX5YtW5Z169Zl7dq1Q1o7AACjR9eHyVJKq5Ty30spX2mvv6yUcnsp5b5SynXtbZeUUu4qpfyf9mMDvhe0lHJOKeWD7eV3/nAZRpIDDjggt9xyS8aNG5ck2bZtW9797ndn8uTJ+au/+qsX3DaQVquV+fPnJ0lmz56dpUuXDv0BAAAwKoyEG/BMS3JNrfW/ttcXJnmk1npJKWVJKeWE9vb7a62XtwPmryb5XwPMdXuS3iRXJPn1JL+z+4BSysL2c+SQQw7Nxcc/v2+Phh/JYWP6b8KzP2u1Wrusb9y4Ma1WK3feeWde/vKX541vfGNuu+22/N3f/V0OOuigPba97W1v22POxx57LE899VRarVaefPLJrF27NkceeeQL1rBp06Y96qAz9KI76EP30IvuoA/dQR+6x2jvxUgIk4/UWpfstH5MkjeUUnqSTEhyeHv7Q+3vDyeZPNBEtdbvlVL+uZTypiQ/UWv9lwHGXJ/k+iR51VFH14+vGgkv0ej3/uOfz/7ei8fP6tllfcKECenp6cntt9+eD33oQ3nLW96Sn//5n89FF12UV7ziFXts6+np2WPOO+64I8cdd1ymTZuW9evXZ9u2bQOO+6FWq7XXxxk+etEd9KF76EV30IfuoA/dY7T3ousvc02yabf1NUmurrX2JFmU5In29pPb31+b5LG9zHdTks8l+ct9WCN0zNFHH51169YlSZYvX54jjjhiwG0DmTp16o5LW1euXJnJkycPS80AAIx8I/FUzw1JPldKOSfJvyX5zfb217fv3Pp0kr/ey/5fTfKSJEv2MgZGjAULFuTcc8/NF7/4xWzbti233357xo0bt8e21atX5+abb97lrq3z5s3LjBkz8tRTT+Wuu+7KAw880MEjAQBgJOn6MNk+A7nz+uYk83feVkpJkqtqra297V9K+en0B83PtOfZqzEvOSBrfFB8V2i1Wntc5rm/++H19z/1Uz+V2267bY/Hd992+OGH7/HxH+PGjUur1Upvb28uuOCCjB8/fsjqBQBgdOn6MDkYtdZLBjlufZI3DG01MLJMnDhxxx1dAQBgsEbCeyYBAADoMsIkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjR3Y6QK62ZZtfZn8wTs7XcaI8vgVcztdAgAAMAycmQQAAKCxERsmSynjSyn3lFJapZS3droeAACA/clIvsx1SpK/r7Uu6nQh7OnP/uzPcssttyRJNm7cmFNOOSVPPvlk/vVf/zVTp07Nn//5n7/gvgsWLMjq1aszd+7cLFqkvQAA0I1G5JnJUsofJLkmyW+1z0xeV0rpaT/2zvbXJaWUj5RS7iulfLOUMqmUMqaU8tftbXeUUkZymO5q733ve9NqtdJqtTJjxowcfvjhOeuss7J8+fJ873vfy/Llywfcb8mSJenr68uyZcuybt26rF27dpgrBwAABmNEhqla6ydKKSuT9NRaLymlXPICQ4+utc4spVyc5NQka5Jsb2/7lSRjk2zceYdSysIkC5PkkEMOzcXHPz9UhzEqtVqtXdafffbZrFq1Kv/5P//n/M3f/E3Gjx+fRx55JE888UQ2bdq0x/6f//znc/LJJ6fVauWVr3xlPv3pT2fOnDnZtGnTHnMz/PShe+hFd9CH7qEX3UEfuoM+dI/R3osRGSZfxJgkW9rLN7W/P5HkpUlWJHmklHJ3krVJ/nb3nWut1ye5PkleddTR9eOrRuNLNHQeP6tnl/UPfehD+eM//uMcddRRufDCC7NixYpMmzYtZ5xxRl7ykpfssf/ixYszZ86cTJkyJVu3bs2KFSvS09OTVquVnp6ePcYzvPShe+hFd9CH7qEX3UEfuoM+dI/R3osReZnrALYmObS9/Jadtm/ebdyUJF+rtc5OMjHJjGGobb+1ffv23Hvvvenp6cmll16aT33qU7n44ovzmte8Jp/73OcG3Gfs2LHZsqX/bwGbNm3K9u3bh7NkAABgkEZLmPxSkveVUj6V5Dt7Gfd4kt8vpfx9kklJBn7jHvvE/fffn1NOOSWllGzYsCGrVq1KX19fvv71r6eUMuA+U6dOzdKlS5MkK1euzOTJk4exYgAAYLBG7DWctdZWklZ7+ZEkM/cy9sadVk8byrr4D1/5ylcyc2Z/Wy688MKcc845+da3vpXp06fnzDPPzOrVq3PzzTfnsssu27HPvHnzMmPGjDz11FO566678sADD3SqfAAAYC9GbJgcDmNeckDWXDG302WMWJdffvmO5ZNPPjmPPvroLo8fe+yxuwTJJBk3blxarVZ6e3tzwQUXZPz48cNSKwAA0IwwSdeZOHFi5s+f3+kyAACAvRgt75kEAABgGAmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjQmTAAAANCZMAgAA0JgwCQAAQGPCJAAAAI0JkwAAADQmTAIAANCYMAkAAEBjwiQAAACNCZMAAAA0JkwCAADQmDAJAABAY8IkAAAAjR3Y6QK62ZZtfZn8wTs7XcagPH7F3E6XAAAA7EecmQQAAKAxYXKUW79+fXp7e/Pcc891uhQAAGAU6aowWUqZXErpGWD71T/KuP3VM888k9e+9rXZsGFDTj/99Dz44IOZNWtWnn322RfcZ8GCBZk+fXouu+yyYawUAAAYqboqTCaZnKRn94211j/8Ecftlz7wgQ9ky5Ytefjhh3PVVVfloosuymmnnZYVK1YMOH7JkiXp6+vLsmXLsm7duqxdu3aYKwYAAEaaUmvdtxOWckmSU5K8LMmzSc5K8rkkRyR5Lsmv11q3lVJaSf4qyTm11hNKKX+Q5JwkE5I8nuTttdZn23O2aq097eVBjWuvX5vkxCQbk/x2kl9NMqX9NSnJ/FrrI7vVvzDJwiQ55JBDp1589Q375HUZascfPj5JsmLFirRarTzxxBO5+uqrkyQrV67MZz/72Vx++eU5+OCD99j3mmuuycknn5xp06blnnvuyQ9+8IPMmTNnOMt/UZs2bcrYsWM7XcZ+Tx+6h150B33oHnrRHfShO+hD9xgNvZg1a9ZDtdaTBnpsqO7men+t9fJSynVJ5iW5M8kt6Q+Vr0vy9SQ/m6TWWk9I/8InSikrk/TUWi95oYkHO66UcnqSn6y1ziilvDPJ/5fkn5K8PsmsJGcm+ZUku4TJWuv1Sa5PklcddXT9+KqRccPbx8/qydatW3PppZfmjjvuyLx589LT05Naa2677bZMnjw5p556asaMGbPHvosXL86cOXMyZcqUbN26NStWrEhPT8/wH8RetFqtrqtpf6QP3UMvuoM+dA+96A760B30oXuM9l4M1WWuD7W/P5zklUlOT3JbkqOS/DDNfDfJNUP0/ElybPpDa5I8kOTn28t/WWvdluSJJC8dwucfdldccUXOO++8TJgwYce2Ukquu+66nHDCCfnSl7404H5jx47Nli1bkvT/9WT79u3DUS4AADCCDVWYPLn9/bVJnk//2b+3Jfn2TmP+vda6e2rZkv7LY1NKKXuZfzDjHk0yrb08rb2eJJsHUf+I9NWvfjXXXXddenp68s1vfjOllNx0001Jko0bN+4SMnc2derULF26NEn/JbGTJ08epooBAICRaqjC5Ovb74mckOSrSd6RZGmSn05y+F72+0aSY0op97f3+ZHH1VrvTLKllLI0ya8l+e8Nj2HEue+++9JqtdJqtXLiiSdm/fr1Wbx4cWbOnJm+vr7Mnj07q1evzqJFi3bZb968eVm8eHHOP//83HrrrZk7d26HjgAAABgphuoNgVfVWls7rZ+w+4Cdb5Sz07at6b9Jzl7HNhj3vt2G3LjTY60krezFmJcckDVXjMxg1Wq1kiS9vb27bD/22GP3+PiPcePGpdVqpbe3NxdccEHGjx8/XGUCAAAj1D4Pk3u7KQ7da+LEiZk/f36nywAAAEaIbvucSQAAAEYAYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoDFhEgAAgMaESQAAABoTJgEAAGhMmAQAAKAxYRIAAIDGhEkAAAAaEyYBAABoTJgEAACgMWESAACAxoRJAAAAGiu11k7X0LVKKd9LsqbTdZAkOSTJc50uAn3oInrRHfShe+hFd9CH7qAP3WM09OKIWuuhAz1w4HBXMsKsqbWe1OkiSEopy/Wi8/She+hFd9CH7qEX3UEfuoM+dI/R3guXuQIAANCYMAkAAEBjwuTeXd/pAthBL7qDPnQPvegO+tA99KI76EN30IfuMap74QY8AAAANObMJAAAAI0JkwBdrJTy06WUXyqlHNLpWgAAdiZMvoBSymdKKctKKYs6Xcv+pJRyWCnl/p3W9+iD3gytUsr4UspdpZS7Syl3lFJeqg+dUUqZmOSvk5yc5N5SyqF60Tnt30/faC/rwzArpRxYSnmilNJqfx2vD51VSvlkKeWM9rJeDLNSynt3+nn4Zinlz/WhM0opE0spf1NKWV5K+fP2tv2iF8LkAEopb0tyQK11epKjSimv7nRN+4P2f5z/IsnB7fU9+qA3w+KsJFfVWmcneTrJb0QfOuWEJOfXWj+S5CtJTo1edNLHkozxu6ljTkjyl7XWnlprT5JXRx86ppQyI8mkWuuX/Ux0Rq31z3b6ebg/yT9HHzrlvyT5QvvzJH+qlHJB9pNeCJMD60lya3v57iRv7Fwp+5W+JO9I8m/t9Z7s2YeBtrEP1Vo/WWvtba8emuS3og8dUWv9P7XWB0opM9N/dvK06EVHlFJOTbI5/X9g6Yk+dMK0JKeXUh4spXwmyZujDx1RSnlJkhuSPF5K+dX4meioUsrhSQ5L8sroQ6d8J8lxpZQJSX4uyZHZT3ohTA7s4CTfbi+vT/8PKEOs1vpvtdbv7rRpoD7ozTAppUxPMjHJk9GHjimllPT/kWVDkhq9GHallJcm+XCSD7Y3+d3UGf+Q5M211pOTvCTJnOhDp/x2ktVJ/lv6/9D1u9GLTvrdJH8Wv5s6aWmSI5L8fpJ/TPLS7Ce9ECYHtinJmPby2HidOmWgPujNMCil/HSSa5OcG33oqNrvd5M8nOQN0YtO+GCST9ZaN7bX/Ux0xsO11v/XXl6e5JDoQ6e8Nsn1tdank3w+yX3Ri44opfxEkllJWvG7qZP+OMnv1Fr/JMk/JfnN7Ce9GBUHMQQeyn+cep6S5PHOlbJfG6gPejPE2mdhbktyYa31W9GHjiml/H+llN9ur05IckX0ohPenOR3SymtJCcmOSP60AmLSylTSikHJJmX/rMx+tAZjyU5qr18UpLJ0YtOmZHk67X/g+P9e905E5Mc3/79dEr2o3+vD+x0AV3qfye5v5TyivRfRjOts+Xst/539uxDHWAb+9aCJK9LclEp5aIkn0vyX/ShI65Pcmsp5V1JHkn/z8R9ejG8aq0zf7jcDpS/Er+bOuFPktycpCT5Uvwb0UmfSfLZUspvpP+S454kX9KLjjgt/WeGEz8TnfTR9P9/6Ygky5L8j+wnvSj9f8hgd+07i/5Skvval3HQAQP1QW+Gnz50D73oDvrQHfShe+hFd9CH7rG/9EKYBAAAoDHvmQQAAKAxYRIAAIDGhEkAAAAaEyYBYAiUUi4ppfxjKaXV/vq9TtcEAPuSjwYBgKHzkVrr5ztdBAAMBWESADqolDImyW1JxiX5TpK3p//f5xuTvDLJxiTzk/S1t70iyb8kOafWurX92Zf/kOSEWutppZSXJbkpycuTrKq1/u4wHg4A+xGXuQLA0LmofYnrJ/cy5tgk22utM9P/oddjkyxMsrLW+sYk/yvJcUneneSRWuubkqxNcm57/2lJltVaT2uvL2yPm5nkZ0spJ+zzowKACJMAMJQ+UmvtqbWet5cxK5I8Ukq5O8lpSf49yWuSPNh+/Mb0n3k8NsnX29seSPLz7eVHaq1LdprvmCRvbZ+xPCrJ4fvgOABgD8IkAHTWlCRfq7XOTjIxyYwk/5Tk9e3HP5TkXUkeTf9ZyLS/P9pe3rTbfGuSXF1r7UmyKMkTQ1Y5APs1YRIAOuvxJL9fSvn7JJOSLE9yQ5LXtc8uvi7J4iSfTvILpZT7krw6/WcsB3JDkjntcb+T5MmhLB6A/VeptXa6BgAAAEYYZyYBAABoTJgEAACgMWESAACAxoRJAAAAGhMmAQAAaEyYBAAAoLH/H/eU01gXrSd4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb.plot_importance(xgbFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a2cf8e",
   "metadata": {},
   "source": [
    "#### 对训练集和测试集进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aa7e74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbFinal_train_preds = xgbFinal.predict(trainDMat)\n",
    "xgbFinal_test_preds = xgbFinal.predict(testDMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d491c56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4428,)\n",
      "(1477,)\n"
     ]
    }
   ],
   "source": [
    "print(xgbFinal_train_preds.shape)\n",
    "print(xgbFinal_test_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576948ab",
   "metadata": {},
   "source": [
    "#### 计算MSE均方误差和RMSE均方根误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b54337bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "MSE Train : 1367053527.560016\n",
      "MSE Test: 25397408806.300060\n",
      "RMSE Train: 36973.686962\n",
      "RMSE Test: 159365.645000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Report\")\n",
    "print(\"MSE Train : %f\" % mean_squared_error(y_train, xgbFinal_train_preds))\n",
    "print(\"MSE Test: %f\" % mean_squared_error(y_test, xgbFinal_test_preds))\n",
    "print(\"RMSE Train: %f\" % mean_squared_error(y_train, xgbFinal_train_preds)**0.5)\n",
    "print(\"RMSE Test: %f\" % mean_squared_error(y_test, xgbFinal_test_preds)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86124398",
   "metadata": {},
   "source": [
    "将模型打包成pkl文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgbFinal, open(\"xgbFinal.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = pd.DataFrame(xgbFinal_train_preds)\n",
    "test_preds = pd.DataFrame(xgbFinal_test_preds)\n",
    "train_preds.columns = ['views']\n",
    "test_preds.columns = ['views']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ed9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_preds.to_csv('XGBoost_Train_Preds.csv', sep=',')\n",
    "# test_preds.to_csv('XGBoost_Test_Preds.csv', sep=',')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
